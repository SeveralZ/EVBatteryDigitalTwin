{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csqCVbe2HKPR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "sPHPaioJLKIK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYkZaViGHHvm",
        "outputId": "e15d6c60-eee9-4d8c-bb82-ec96547d5225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.10/dist-packages (2024.8.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (2024.6.1)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]) (8.5.0)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n",
            "  Downloading dask_expr-1.1.14-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.11-py3-none-any.whl.metadata (2.5 kB)\n",
            "  Downloading dask_expr-1.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]) (14.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading dask_expr-1.1.10-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dask-expr\n",
            "Successfully installed dask-expr-1.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors optuna xgboost lightgbm catboost tensorflow plotly dask[dataframe]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation of Synthetic Data"
      ],
      "metadata": {
        "id": "FDraL3MgLdIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports various Python libraries for data analysis, machine learning, and web development. It also defines constants related to battery assembly, including stages, cell types, welding methods, defect types, and parameter thresholds."
      ],
      "metadata": {
        "id": "oHM88Hz7LGuh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHpFyHA66fmd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, IsolationForest\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "from sklearn.metrics import confusion_matrix, mean_squared_error, r2_score, accuracy_score\n",
        "from scipy import stats\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "import json\n",
        "from google.colab.output import eval_js\n",
        "import random\n",
        "\n",
        "# Constants\n",
        "ASSEMBLY_STAGES = ['Pre_Assembly', 'Insulation_Tensioning', 'Electrical_Contacting', 'Slave_Board_Mounting', 'Housing_Cover_Mounting']\n",
        "CELL_TYPES = ['Prismatic', 'Pouch', 'Cylindrical']\n",
        "WELDING_METHODS = ['Laser', 'Ultrasonic', 'Resistance']\n",
        "DEFECT_TYPES = ['No_Defect', 'Minor_Defect', 'Major_Defect', 'Critical_Defect']\n",
        "THRESHOLDS = {\n",
        "    'Cell_Voltage': (3.2, 4.2),\n",
        "    'Cell_Impedance': (0.5, 2.0),\n",
        "    'Cell_Capacity': (2000, 5000),  # mAh\n",
        "    'Compression_Force': (500, 2000),  # N\n",
        "    'Welding_Current': (100, 300),  # A\n",
        "    'Welding_Time': (0.1, 1.0),  # s\n",
        "    'Torque': (5, 15),  # Nm\n",
        "    'Assembly_Time': (10, 30),  # min\n",
        "    'Leakage_Rate': (0, 0.1),  # cm³/min\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EV Module Assembly Analyzer Class\n",
        "\n",
        "This code defines an `EVModuleAssemblyAnalyzer` class for analyzing EV module assembly processes. It includes methods for data generation, model training, anomaly detection, result visualization, and process optimization using various machine learning techniques. The class also provides functionality for running what-if scenarios and detecting errors in the assembly process."
      ],
      "metadata": {
        "id": "iWZIjympMbN2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6u1PC5rq6sUX",
        "outputId": "6bf43530-68c3-40e7-f3c2-c8c2bc699456"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 16:57:51,169] A new study created in memory with name: no-name-b434e3d9-b24b-468c-8240-8d7426cdc304\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating and preprocessing data...\n",
            "Training models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 16:58:03,110] Trial 0 finished with value: -32.501881704439 and parameters: {'n_estimators': 92, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 0 with value: -32.501881704439.\n",
            "[I 2024-09-19 16:58:10,968] Trial 1 finished with value: -32.49948482736541 and parameters: {'n_estimators': 57, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 1 with value: -32.49948482736541.\n",
            "[I 2024-09-19 16:58:25,149] Trial 2 finished with value: -32.11917914657607 and parameters: {'n_estimators': 177, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 2 with value: -32.11917914657607.\n",
            "[I 2024-09-19 16:58:47,681] Trial 3 finished with value: -32.440848640125296 and parameters: {'n_estimators': 163, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 2 with value: -32.11917914657607.\n",
            "[I 2024-09-19 16:58:55,033] Trial 4 finished with value: -32.37003781249895 and parameters: {'n_estimators': 59, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 2 with value: -32.11917914657607.\n",
            "[I 2024-09-19 16:59:15,260] Trial 5 finished with value: -32.24981499206191 and parameters: {'n_estimators': 189, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 2 with value: -32.11917914657607.\n",
            "[I 2024-09-19 16:59:25,474] Trial 6 finished with value: -31.997468619853606 and parameters: {'n_estimators': 184, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 16:59:40,250] Trial 7 finished with value: -32.4873592565752 and parameters: {'n_estimators': 108, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 16:59:47,927] Trial 8 finished with value: -32.01450003427517 and parameters: {'n_estimators': 138, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 16:59:56,067] Trial 9 finished with value: -32.13779148119714 and parameters: {'n_estimators': 106, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:00:04,285] Trial 10 finished with value: -32.012792293791904 and parameters: {'n_estimators': 142, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:00:12,130] Trial 11 finished with value: -32.01216899584653 and parameters: {'n_estimators': 153, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:00:30,430] Trial 12 finished with value: -32.110007699186056 and parameters: {'n_estimators': 197, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:00:42,107] Trial 13 finished with value: -32.07858430048056 and parameters: {'n_estimators': 161, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:00:52,749] Trial 14 finished with value: -32.01506096016783 and parameters: {'n_estimators': 158, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:01:09,692] Trial 15 finished with value: -32.211837798228565 and parameters: {'n_estimators': 183, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:01:19,589] Trial 16 finished with value: -32.07517482697072 and parameters: {'n_estimators': 136, 'max_depth': 4, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:01:32,087] Trial 17 finished with value: -32.07006902104531 and parameters: {'n_estimators': 169, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:01:45,974] Trial 18 finished with value: -32.216652012471414 and parameters: {'n_estimators': 149, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:01:52,677] Trial 19 finished with value: -32.0141320621275 and parameters: {'n_estimators': 124, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 6 with value: -31.997468619853606.\n",
            "[I 2024-09-19 17:01:57,719] A new study created in memory with name: no-name-9b0aaf6e-24ac-4e3b-8481-5e39f3866a14\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:01:59,418] Trial 0 finished with value: -32.309872985416234 and parameters: {'max_depth': 3, 'learning_rate': 0.03579389593362616, 'n_estimators': 102, 'min_child_weight': 5, 'subsample': 0.6646228197753389}. Best is trial 0 with value: -32.309872985416234.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:02:03,234] Trial 1 finished with value: -31.901346124262282 and parameters: {'max_depth': 4, 'learning_rate': 0.0025766176511115993, 'n_estimators': 173, 'min_child_weight': 3, 'subsample': 0.6595942546130742}. Best is trial 1 with value: -31.901346124262282.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:02:26,031] Trial 2 finished with value: -35.131631292732905 and parameters: {'max_depth': 8, 'learning_rate': 0.07387239056028026, 'n_estimators': 195, 'min_child_weight': 2, 'subsample': 0.8373342989673945}. Best is trial 1 with value: -31.901346124262282.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:02:37,685] Trial 3 finished with value: -35.50749209513156 and parameters: {'max_depth': 8, 'learning_rate': 0.09249819515679338, 'n_estimators': 79, 'min_child_weight': 1, 'subsample': 0.7793659942790115}. Best is trial 1 with value: -31.901346124262282.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:02:51,139] Trial 4 finished with value: -33.7811212180376 and parameters: {'max_depth': 7, 'learning_rate': 0.03384718552507001, 'n_estimators': 149, 'min_child_weight': 2, 'subsample': 0.7911135798714418}. Best is trial 1 with value: -31.901346124262282.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:02:54,847] Trial 5 finished with value: -31.930987676561983 and parameters: {'max_depth': 6, 'learning_rate': 0.004766042463304097, 'n_estimators': 83, 'min_child_weight': 4, 'subsample': 0.8970523351722182}. Best is trial 1 with value: -31.901346124262282.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:07,278] Trial 6 finished with value: -31.88908236907632 and parameters: {'max_depth': 7, 'learning_rate': 0.0018426838750251178, 'n_estimators': 174, 'min_child_weight': 4, 'subsample': 0.717368492145515}. Best is trial 6 with value: -31.88908236907632.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:09,056] Trial 7 finished with value: -35.72041920421172 and parameters: {'max_depth': 3, 'learning_rate': 0.26885292493113133, 'n_estimators': 103, 'min_child_weight': 2, 'subsample': 0.9096239639703271}. Best is trial 6 with value: -31.88908236907632.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:18,915] Trial 8 finished with value: -31.947366340040496 and parameters: {'max_depth': 8, 'learning_rate': 0.005134054428397651, 'n_estimators': 91, 'min_child_weight': 3, 'subsample': 0.7382399900745845}. Best is trial 6 with value: -31.88908236907632.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:21,737] Trial 9 finished with value: -31.97783173200517 and parameters: {'max_depth': 5, 'learning_rate': 0.006169895124199516, 'n_estimators': 79, 'min_child_weight': 2, 'subsample': 0.9292844747859921}. Best is trial 6 with value: -31.88908236907632.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:28,999] Trial 10 finished with value: -31.886801979997642 and parameters: {'max_depth': 6, 'learning_rate': 0.001389065192947231, 'n_estimators': 144, 'min_child_weight': 5, 'subsample': 0.6144383166023067}. Best is trial 10 with value: -31.886801979997642.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:35,676] Trial 11 finished with value: -31.871016788286546 and parameters: {'max_depth': 6, 'learning_rate': 0.0010920946389132108, 'n_estimators': 141, 'min_child_weight': 5, 'subsample': 0.6018474013223754}. Best is trial 11 with value: -31.871016788286546.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:42,407] Trial 12 finished with value: -31.865512030031688 and parameters: {'max_depth': 6, 'learning_rate': 0.0010594733886443398, 'n_estimators': 135, 'min_child_weight': 5, 'subsample': 0.6027944306990467}. Best is trial 12 with value: -31.865512030031688.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:47,526] Trial 13 finished with value: -31.88510910586145 and parameters: {'max_depth': 5, 'learning_rate': 0.0010788922350887563, 'n_estimators': 124, 'min_child_weight': 5, 'subsample': 0.999949944512334}. Best is trial 12 with value: -31.865512030031688.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:03:52,976] Trial 14 finished with value: -32.31834017665538 and parameters: {'max_depth': 6, 'learning_rate': 0.012982426456245847, 'n_estimators': 127, 'min_child_weight': 4, 'subsample': 0.6037762180147297}. Best is trial 12 with value: -31.865512030031688.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:04:02,639] Trial 15 finished with value: -32.25026077070499 and parameters: {'max_depth': 7, 'learning_rate': 0.010934354817888998, 'n_estimators': 129, 'min_child_weight': 5, 'subsample': 0.6695259552584069}. Best is trial 12 with value: -31.865512030031688.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:04:07,433] Trial 16 finished with value: -31.906526104052077 and parameters: {'max_depth': 5, 'learning_rate': 0.0035873351305672316, 'n_estimators': 156, 'min_child_weight': 4, 'subsample': 0.7173335862309443}. Best is trial 12 with value: -31.865512030031688.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:04:11,135] Trial 17 finished with value: -31.85730094779094 and parameters: {'max_depth': 4, 'learning_rate': 0.0011018827213778062, 'n_estimators': 58, 'min_child_weight': 5, 'subsample': 0.6002333815216437}. Best is trial 17 with value: -31.85730094779094.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:04:12,717] Trial 18 finished with value: -41.28122940990053 and parameters: {'max_depth': 4, 'learning_rate': 0.3811815413166985, 'n_estimators': 63, 'min_child_weight': 3, 'subsample': 0.6474664424282335}. Best is trial 17 with value: -31.85730094779094.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
            "[I 2024-09-19 17:04:14,083] Trial 19 finished with value: -31.862376297033627 and parameters: {'max_depth': 4, 'learning_rate': 0.0024328572331115553, 'n_estimators': 57, 'min_child_weight': 4, 'subsample': 0.847053045884266}. Best is trial 17 with value: -31.85730094779094.\n",
            "[I 2024-09-19 17:04:14,549] A new study created in memory with name: no-name-7fbe8a28-da07-4f39-8cc1-e85d209da73d\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000488 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000666 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001331 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:20,696] Trial 0 finished with value: -31.941460552498967 and parameters: {'num_leaves': 42, 'learning_rate': 0.0014796764555403258, 'n_estimators': 140, 'min_child_samples': 15, 'subsample': 0.7882966172360444}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000781 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:27,459] Trial 1 finished with value: -32.21408140723831 and parameters: {'num_leaves': 49, 'learning_rate': 0.0063882769745282455, 'n_estimators': 115, 'min_child_samples': 40, 'subsample': 0.6788702307587049}. Best is trial 0 with value: -31.941460552498967.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:35,317] Trial 2 finished with value: -33.163562160408176 and parameters: {'num_leaves': 45, 'learning_rate': 0.017402394976884686, 'n_estimators': 173, 'min_child_samples': 18, 'subsample': 0.9221555459241152}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000489 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000471 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:42,239] Trial 3 finished with value: -32.3390879759108 and parameters: {'num_leaves': 41, 'learning_rate': 0.007633716765470114, 'n_estimators': 127, 'min_child_samples': 15, 'subsample': 0.7174400411328501}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000527 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:46,635] Trial 4 finished with value: -33.6897503771956 and parameters: {'num_leaves': 40, 'learning_rate': 0.038361372485383786, 'n_estimators': 107, 'min_child_samples': 19, 'subsample': 0.9295095474457751}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:54,093] Trial 5 finished with value: -35.17871900844146 and parameters: {'num_leaves': 45, 'learning_rate': 0.06422013350608258, 'n_estimators': 137, 'min_child_samples': 28, 'subsample': 0.8751146615299963}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:04:57,237] Trial 6 finished with value: -32.33548662514245 and parameters: {'num_leaves': 32, 'learning_rate': 0.011177669827347407, 'n_estimators': 94, 'min_child_samples': 46, 'subsample': 0.7782260627439112}. Best is trial 0 with value: -31.941460552498967.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:00,326] Trial 7 finished with value: -32.909389847879474 and parameters: {'num_leaves': 22, 'learning_rate': 0.030406008899263152, 'n_estimators': 119, 'min_child_samples': 6, 'subsample': 0.7835876084880097}. Best is trial 0 with value: -31.941460552498967.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000494 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:05,365] Trial 8 finished with value: -31.95669743291476 and parameters: {'num_leaves': 29, 'learning_rate': 0.0016063127291454154, 'n_estimators': 136, 'min_child_samples': 9, 'subsample': 0.7193744216552923}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000460 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:10,145] Trial 9 finished with value: -37.339950655099955 and parameters: {'num_leaves': 47, 'learning_rate': 0.17923605664377004, 'n_estimators': 82, 'min_child_samples': 24, 'subsample': 0.8390931150079467}. Best is trial 0 with value: -31.941460552498967.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000475 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000452 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:17,184] Trial 10 finished with value: -31.875302841462357 and parameters: {'num_leaves': 37, 'learning_rate': 0.0010884960333518308, 'n_estimators': 192, 'min_child_samples': 30, 'subsample': 0.6008898715358877}. Best is trial 10 with value: -31.875302841462357.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000455 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:26,290] Trial 11 finished with value: -31.868425484824485 and parameters: {'num_leaves': 37, 'learning_rate': 0.0011558121536818012, 'n_estimators': 199, 'min_child_samples': 32, 'subsample': 0.6059387981650215}. Best is trial 11 with value: -31.868425484824485.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000485 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:34,584] Trial 12 finished with value: -31.866313766240893 and parameters: {'num_leaves': 36, 'learning_rate': 0.0010301999737276392, 'n_estimators': 198, 'min_child_samples': 32, 'subsample': 0.6084834503614382}. Best is trial 12 with value: -31.866313766240893.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:41,860] Trial 13 finished with value: -32.1010481652922 and parameters: {'num_leaves': 34, 'learning_rate': 0.0033645634846794998, 'n_estimators': 198, 'min_child_samples': 38, 'subsample': 0.6136349910237288}. Best is trial 12 with value: -31.866313766240893.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:43,419] Trial 14 finished with value: -31.868271701147222 and parameters: {'num_leaves': 26, 'learning_rate': 0.003312413577007947, 'n_estimators': 50, 'min_child_samples': 33, 'subsample': 0.6500860061412196}. Best is trial 12 with value: -31.866313766240893.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000470 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000483 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:44,963] Trial 15 finished with value: -31.883250265434633 and parameters: {'num_leaves': 25, 'learning_rate': 0.003637618004454813, 'n_estimators': 52, 'min_child_samples': 36, 'subsample': 0.655477576078932}. Best is trial 12 with value: -31.866313766240893.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000457 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:46,911] Trial 16 finished with value: -31.881260157502915 and parameters: {'num_leaves': 28, 'learning_rate': 0.0030011305096875876, 'n_estimators': 51, 'min_child_samples': 50, 'subsample': 0.6694905792694644}. Best is trial 12 with value: -31.866313766240893.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001372 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000480 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:52,177] Trial 17 finished with value: -31.985451273829277 and parameters: {'num_leaves': 21, 'learning_rate': 0.002750646763649677, 'n_estimators': 161, 'min_child_samples': 25, 'subsample': 0.7125143230899476}. Best is trial 12 with value: -31.866313766240893.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000578 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:54,583] Trial 18 finished with value: -40.183970747536556 and parameters: {'num_leaves': 29, 'learning_rate': 0.36198986895325264, 'n_estimators': 72, 'min_child_samples': 44, 'subsample': 0.9713337568204727}. Best is trial 12 with value: -31.866313766240893.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:05:59,869] Trial 19 finished with value: -32.20169165546007 and parameters: {'num_leaves': 32, 'learning_rate': 0.0056864809667792095, 'n_estimators': 158, 'min_child_samples': 31, 'subsample': 0.6453755050906788}. Best is trial 12 with value: -31.866313766240893.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.015935\n",
            "[LightGBM] [Info] Start training from score -2.275456\n",
            "[LightGBM] [Info] Start training from score -1.962548\n",
            "[LightGBM] [Info] Start training from score -0.345664\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.070965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:06:03,966] A new study created in memory with name: no-name-d0d3c7e6-c483-440b-8076-fdf7d451abba\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:06:36,466] Trial 0 finished with value: -40.475840207822756 and parameters: {'depth': 8, 'learning_rate': 0.4926296554018983, 'iterations': 175}. Best is trial 0 with value: -40.475840207822756.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:06:42,656] Trial 1 finished with value: -32.043025762016654 and parameters: {'depth': 6, 'learning_rate': 0.028456957315184796, 'iterations': 93}. Best is trial 1 with value: -32.043025762016654.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:06:46,076] Trial 2 finished with value: -32.205653861934856 and parameters: {'depth': 5, 'learning_rate': 0.08325375731370355, 'iterations': 67}. Best is trial 1 with value: -32.043025762016654.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:04,058] Trial 3 finished with value: -33.234608167666146 and parameters: {'depth': 7, 'learning_rate': 0.07653251217990308, 'iterations': 155}. Best is trial 1 with value: -32.043025762016654.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:06,633] Trial 4 finished with value: -33.63941088239172 and parameters: {'depth': 5, 'learning_rate': 0.24444876705811647, 'iterations': 58}. Best is trial 1 with value: -32.043025762016654.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:09,263] Trial 5 finished with value: -31.970173208764066 and parameters: {'depth': 5, 'learning_rate': 0.04251824537963218, 'iterations': 53}. Best is trial 5 with value: -31.970173208764066.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:18,238] Trial 6 finished with value: -33.07827818698749 and parameters: {'depth': 6, 'learning_rate': 0.07909036522560688, 'iterations': 137}. Best is trial 5 with value: -31.970173208764066.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:26,531] Trial 7 finished with value: -38.15538417330278 and parameters: {'depth': 5, 'learning_rate': 0.33140692142615735, 'iterations': 192}. Best is trial 5 with value: -31.970173208764066.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:29,986] Trial 8 finished with value: -32.92638179617183 and parameters: {'depth': 4, 'learning_rate': 0.11049523480190712, 'iterations': 151}. Best is trial 5 with value: -31.970173208764066.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:07:42,116] Trial 9 finished with value: -31.871247813499423 and parameters: {'depth': 7, 'learning_rate': 0.0036331471436554514, 'iterations': 114}. Best is trial 9 with value: -31.871247813499423.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:08:00,291] Trial 10 finished with value: -31.860114156197533 and parameters: {'depth': 8, 'learning_rate': 0.002762657244671073, 'iterations': 104}. Best is trial 10 with value: -31.860114156197533.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:08:21,029] Trial 11 finished with value: -31.86161188476178 and parameters: {'depth': 8, 'learning_rate': 0.003202190031291525, 'iterations': 106}. Best is trial 10 with value: -31.860114156197533.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:08:39,521] Trial 12 finished with value: -31.857471662005487 and parameters: {'depth': 8, 'learning_rate': 0.001990916879400874, 'iterations': 105}. Best is trial 12 with value: -31.857471662005487.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:08:55,484] Trial 13 finished with value: -31.895809162965563 and parameters: {'depth': 8, 'learning_rate': 0.007577542168446494, 'iterations': 87}. Best is trial 12 with value: -31.857471662005487.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:09:09,165] Trial 14 finished with value: -31.851989642768004 and parameters: {'depth': 7, 'learning_rate': 0.0012115710091633348, 'iterations': 129}. Best is trial 14 with value: -31.851989642768004.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:09:22,849] Trial 15 finished with value: -31.855207378782296 and parameters: {'depth': 7, 'learning_rate': 0.0015449131570332525, 'iterations': 129}. Best is trial 14 with value: -31.851989642768004.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:09:37,272] Trial 16 finished with value: -31.854053513159947 and parameters: {'depth': 7, 'learning_rate': 0.0011481014950235294, 'iterations': 139}. Best is trial 14 with value: -31.851989642768004.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:09:54,616] Trial 17 finished with value: -31.8534718098216 and parameters: {'depth': 7, 'learning_rate': 0.0010303434119225282, 'iterations': 151}. Best is trial 14 with value: -31.851989642768004.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:10:03,692] Trial 18 finished with value: -31.937158065235813 and parameters: {'depth': 6, 'learning_rate': 0.008195176545536783, 'iterations': 167}. Best is trial 14 with value: -31.851989642768004.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
            "[I 2024-09-19 17:10:17,000] Trial 19 finished with value: -31.927490563036855 and parameters: {'depth': 7, 'learning_rate': 0.009049487005095794, 'iterations': 123}. Best is trial 14 with value: -31.851989642768004.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detecting anomalies...\n",
            "Detected 50 anomalies.\n",
            "Analyzing and generating recommendations...\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "Visualizing results...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f6fb5ed1-85eb-4527-b86d-43336ce56f11\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6fb5ed1-85eb-4527-b86d-43336ce56f11\")) {                    Plotly.newPlot(                        \"f6fb5ed1-85eb-4527-b86d-43336ce56f11\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[0.05941733137141885,0.06160711328757705,0.06227571493511739,0.065868176592982,0.06653172958154921,0.06957255356895292,0.07225366633936023,0.07877849760727766,0.09897263308298412,0.1322818059296689],\"xaxis\":\"x\",\"y\":[\"Leakage_Rate\",\"Compression_Force\",\"Cell_Voltage\",\"Welding_Time\",\"Assembly_Time\",\"Welding_Current\",\"Production_Rate\",\"Cell_Capacity\",\"Torque\",\"Room_Temperature\"],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Top 10 Feature Importance\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f6fb5ed1-85eb-4527-b86d-43336ce56f11');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"233d1add-c330-410c-b012-259313607aad\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"233d1add-c330-410c-b012-259313607aad\")) {                    Plotly.newPlot(                        \"233d1add-c330-410c-b012-259313607aad\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"Critical_Defect\",\"Major_Defect\",\"Minor_Defect\",\"No_Defect\"],\"y\":[\"Critical_Defect\",\"Major_Defect\",\"Minor_Defect\",\"No_Defect\"],\"z\":[[0,0,0,41],[0,0,0,104],[0,0,0,149],[0,0,0,706]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Predicted: %{x}\\u003cbr\\u003eActual: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Predicted\"},\"side\":\"top\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Actual\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(247,251,255)\"],[0.125,\"rgb(222,235,247)\"],[0.25,\"rgb(198,219,239)\"],[0.375,\"rgb(158,202,225)\"],[0.5,\"rgb(107,174,214)\"],[0.625,\"rgb(66,146,198)\"],[0.75,\"rgb(33,113,181)\"],[0.875,\"rgb(8,81,156)\"],[1.0,\"rgb(8,48,107)\"]]},\"title\":{\"text\":\"Confusion Matrix - Defect Prediction\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('233d1add-c330-410c-b012-259313607aad');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"170a7e99-54a3-4eb5-a871-b635ff04949e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"170a7e99-54a3-4eb5-a871-b635ff04949e\")) {                    Plotly.newPlot(                        \"170a7e99-54a3-4eb5-a871-b635ff04949e\",                        [{\"hovertemplate\":\"Actual Efficiency=%{x}\\u003cbr\\u003ePredicted Efficiency=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[95.28405927586574,92.13663460012599,94.54260327211948,83.59586424348163,90.89492795770327,83.64944041818978,89.31342028086078,88.19714925777807,88.48005638215646,89.68816419313357,89.61798725150155,82.30546925485702,97.33417665315238,97.46862791703877,80.97973668498467,94.40415414258763,83.86339973229926,90.9161627966871,86.05532043851854,95.6780461202919,84.9726295835971,87.50467600976285,89.29660717625657,88.4256645337992,99.76888022914423,89.35885125310587,80.5691879803636,97.35258406355184,98.82219905998923,80.4359274784849,83.35023089213692,89.87277800363373,80.74214435739952,87.1879771522437,91.13757412567809,85.45321983520375,88.16262547340625,98.62034497125681,97.90440724191168,90.61932257950366,89.68044073131323,96.06853588543697,84.47388520696067,97.40380738814588,97.60375562025614,95.04711044563025,82.09154532610314,90.65049052365106,86.36038135113895,96.51172020453934,90.46583690835875,87.86049230441299,91.85806771896534,84.58733387431354,82.2708313613145,80.94297918514556,95.52100134588822,95.46834983740297,80.78438372384392,96.44970544537267,99.0806887037995,94.45239068010811,86.20818444747171,87.83452675161816,93.43733946103258,91.50055054899498,88.4078728863617,86.92486165633001,92.6637539380923,98.90233240507169,89.85926148750384,81.18593894927847,90.43653468983425,86.41964439020875,92.35614189548947,97.56022843205616,96.7526476760468,97.86162033342514,83.36561282117705,91.67117316817507,83.86897877984985,97.90771426391635,97.0843517892625,91.07540295665058,97.10745520583261,88.29634037617627,96.96550443435714,90.72530951119138,91.4420444194463,98.56937447690554,83.72457807839697,84.1031905946008,87.967287100243,97.66197659847376,97.42738677083825,80.64280987508569,89.40923389904187,82.82017929491823,84.3162135461576,84.69008773981022,90.22997119572771,94.27766797704712,98.331089766666,93.69495391118994,96.80740812175631,95.27425750852228,81.04712015810952,93.31729323980461,90.924598487476,81.06357034217456,81.61633231922296,82.12674421000877,81.38439970531758,83.62364427473563,83.58137458950026,80.36910155047117,82.55563508013351,98.41847890200853,82.68256280093217,89.97690159939042,97.23723908645192,97.58083360137428,84.19204584122967,96.30281566956687,92.6417706773658,81.44621730494167,90.19791580574116,96.82691797945358,85.05369309418197,80.04082953673425,92.28844461893382,85.70370100902856,97.52023886126389,91.34029799738931,97.07145732964057,95.6426965949824,89.04877314810363,96.90240100944573,95.44046630055695,94.17403172026022,80.797028874008,93.80833455784258,93.40240423634945,92.9657550216737,82.19636374179439,90.36825471853307,99.10721593813852,80.98377884124645,85.48571462004676,98.22006683149286,81.05935499960592,91.9094145310961,88.6179530299882,97.62702858451934,86.32224713660895,91.35415977987132,82.01627124551715,94.92987767204998,91.78055188223767,99.67587593505056,91.79954980622583,82.02183191692994,95.35732148988691,88.82635565373572,91.98140095009745,87.0644866608243,81.56331811942131,93.03223806197698,81.23062654329136,94.00946205693037,88.97061311278763,82.97497111385971,98.56941630218614,80.62148748492173,91.97507304853355,99.80206408613554,89.20562031969158,88.56389841916986,85.63072018627915,88.30782358736079,91.39926182751285,95.46581380691264,80.03124984137456,98.28026804963228,93.45250022277853,83.09856622584103,94.0423951008188,87.93163399433367,82.0330757297939,84.20754217814184,86.95381272464377,81.02918562179364,83.55653633236038,80.8294729093144,99.21432001524721,95.75841490185502,94.28882983364534,81.49938135020122,98.0385116073164,95.69698129907042,90.41613251836945,82.4727954908496,99.5655160375816,91.55793483499062,95.1013410610916,94.6988574663702,91.00685518686733,80.4542808185089,81.9929345376367,98.29256596816705,99.42160859485546,84.2839042320293,97.6943605360598,98.75191077341971,89.96211658974933,87.35767744484093,84.2469405116866,87.92333611972224,90.43823196491742,91.62480083882218,87.97522275604219,84.42620753828811,89.30121711726916,89.03063431411334,86.67685414839933,89.04019511540511,92.2939001186552,87.0868771407461,97.82648905269198,80.70974239958618,89.96484499317393,81.9004897039605,96.6120040052971,82.57371514720323,91.3326545054649,80.1335168990698,80.99857620663073,94.15188306182334,92.52526731519036,86.4866791021416,95.95829373131562,99.32866198960281,95.5112903238217,84.47872356490477,95.7970384986325,89.86220828172043,93.55981955271974,82.1094882092763,82.09716202620365,97.62475338147478,92.35628021900624,98.6811835300959,85.03823262840893,93.33777702482735,82.26932330878319,91.94034762627747,81.95867403647907,85.05480298348338,92.32499807323329,94.85996882291323,90.60621253037651,92.93401859330463,86.6530921761097,94.96748988821116,96.11981504320725,83.96870682359452,80.9009268894749,99.84526265760788,82.32027180319503,89.25061915180439,81.29707534332151,85.15892984155604,99.36729190827995,94.23547175554131,87.23880627313498,80.00195801390282,98.90010286677355,80.21407757566656,95.37190961911666,84.29486371528567,81.94970886427518,94.2744009709139,95.677113361086,99.95774694965007,87.63480101083563,84.18100798735804,95.03964285187288,88.29295187379778,82.01360150125299,99.25286341694628,88.13419883010937,88.71837120474302,90.69942544351109,93.37136312496821,83.88492941701668,95.0404740021251,89.40989010931003,88.52580641767577,92.07133538187078,89.41219995959293,95.9942484379753,85.61403551475934,94.85703450798314,90.94647789814516,95.12650538529545,84.6869896105854,80.360340462627,86.68459104104241,94.88307138636638,81.95367290752753,98.48999822914377,88.45076479167683,86.54275794550134,80.46586241847177,92.04716733498769,89.18084853049021,98.20806187099957,85.70435831329462,85.03245111968414,80.93392262329677,94.07687200307909,97.09455737577191,94.49783326153225,82.51028055028299,85.10802111495576,88.99646106807866,93.81061471521406,84.11941888369248,86.3677174800324,91.78369383562244,88.78121420362119,98.88248245267542,89.3254866514661,86.82114484074069,88.34379335849574,92.60971765502049,94.84579527185181,82.2486961942624,89.68768525417883,82.43913929834869,96.77345972030756,95.68838263896498,95.16099274192426,92.11414106793893,99.27159029910722,97.54495853237911,80.01131525450356,96.35740826002342,84.28611819988005,86.29510224119093,81.6303517339898,91.6263935074987,98.62046129540144,84.84265411478928,82.5911656175837,89.35371974681608,84.56586913958168,85.76831796898021,96.65137341319806,81.54346290001497,84.44549893990394,88.77121644389418,90.45303913677557,87.20168592987214,89.15960710369873,80.53014634508108,94.25983872169081,85.6118709735553,91.48682635302453,81.03093597104993,83.11171725637737,95.55768214277573,92.48133952186771,97.80877471856347,89.11949111432666,90.19931530277293,96.58081350656892,91.85091213172818,86.35550316370733,88.83172891740574,99.65911972334796,84.19571542078131,80.06075009035011,98.05638061013411,97.49536399095177,97.12706744358331,97.05905396105996,84.22935609832201,95.2713967262701,83.69325342456892,94.82830374904655,87.88834626751981,86.65173471191596,96.42565893624237,83.65141695137237,99.33267158804092,96.69838186814312,96.27445108156351,92.8596827112708,89.03485166948083,91.11383474125827,96.29195618399478,92.72298389628985,91.3845926173442,87.54133691902715,99.14969730292711,88.78144494575528,89.37904033152536,95.92765612946471,97.66646664555665,97.96643169803683,94.43525328441704,98.45404487195557,85.54251257024178,81.85085899760718,80.9171846734557,96.4240575325274,85.26890938112552,88.00454768988979,95.50002568488773,84.03190059241354,96.98905133678555,81.15999084937508,95.98546798909739,81.57396183270946,87.57323210032041,87.83279221458939,98.26668667067088,87.22500034529224,83.00002350345673,95.86289075243894,99.22979449531888,98.60629963549546,80.63398772766762,97.42959236422456,98.3110051036601,82.13087330119215,98.37600784726932,95.9536800332968,82.90794781074912,95.58520343091708,94.60124120762082,97.96217832322864,82.57206577440549,84.95648099369153,97.1829206395636,99.85030728068409,90.42225222177296,86.7763685838059,80.7088489887811,92.23715855019998,88.05563468889125,85.20521701386058,82.67928951781184,89.03473573197897,89.21173734970726,87.38444302225935,85.51090868089423,90.51058841182419,90.46158923447668,80.26698577007578,82.88656441895338,94.1611519141376,90.3104522994373,84.77270085273756,98.57003122503664,93.99990698079023,83.69589118144951,89.90069955975983,99.42069490324431,99.9119289955089,92.6788294339141,82.47103750058713,95.92738163833283,81.33478964465066,83.48159934265331,90.63539642626831,90.60418471336513,83.89680800789351,83.22205483177714,86.24391067236108,83.194937318795,99.86149619666463,91.92530251494485,88.0707443134577,80.14694731138864,87.15057731523544,95.34244569293094,84.54176941824042,86.80580539662091,97.49889896366352,86.44467075611063,91.99746414371602,99.54785465083857,82.25706021933256,94.40209342438848,87.63053733607028,83.46306717531982,93.52708138661207,88.60852586788435,91.95701781678103,97.91772794408921,93.23201321197276,92.6589202604168,94.37800622427227,92.36124880413252,86.37883550797592,86.42176237871055,88.6654814471431,99.54019139265539,84.0057356934495,89.14596216909757,83.80359248430999,91.09203081231864,88.89167357616024,98.8870943663279,82.68580904000675,82.73407494637496,87.47596919599394,88.92270949154968,80.71304670690122,83.09149890475301,90.02196542837214,96.14047110403163,96.79984380298629,89.38728979210255,90.82156066266415,89.06938170708044,94.10971766577993,83.2998301772654,84.75204036232822,84.61852131118323,94.41442080924867,80.31511795651777,96.63599826772366,86.85142070164187,87.30992817819516,97.65463502297726,84.56078063643875,95.8792107113127,94.0033067406191,95.33672254841822,93.54212771388305,93.58911138583831,86.02341841328699,91.1647631345834,99.29533007432941,94.88024933946393,99.16048700446521,97.52675832972719,98.37533710983395,83.76505341767151,90.4968982430918,93.98516904786443,83.05455340578656,97.05453707152688,98.54159894965983,89.87318016664315,98.97461831160848,91.64627480740539,94.44359417074003,88.79281081896735,93.82662665303532,90.57156574334745,96.33348132935296,98.10106005888845,86.41891446808596,81.61598596970414,89.2962226508605,93.02548178548204,83.94496034346282,93.50988129909355,90.9858865046628,99.84333270475686,81.4184395238392,89.60174104352672,86.6836619515201,90.1151038003491,99.92206297224499,93.32588866938983,82.5700648795462,97.2414573287488,81.6876643034088,90.37647335130998,99.20664460689707,83.75079234989394,88.967577037399,99.73559928257845,80.79526263063627,91.54349433150712,90.67285462127612,92.79021512554024,96.37863609365351,89.30378371264813,84.11485582232459,94.87394541134239,92.95660289637942,93.44913754106904,92.02169446349816,89.60174873129486,87.15169233171727,94.04034452152847,84.6461793723251,89.88609385597076,97.46152271074767,82.1010622588019,93.67848769203147,84.06342863169117,87.76805397254412,84.23432226657798,80.41066737166324,87.8990038579853,93.28422435267544,83.20504088835123,97.91222588017396,94.06358895600115,87.59713898687103,86.90142817041783,90.10276994606929,83.44583790432357,98.03004131954094,82.54715335544886,98.43156808052856,92.42179690561315,80.19771047479362,90.07819051672288,81.77955869230158,87.20103058732053,87.76878761989742,80.59877380661676,97.86529292170843,81.25004984370335,93.92737529446985,96.83287335840318,87.05457510162384,86.35775735771557,95.54566720263286,97.07563612950247,98.38856287159743,85.40418805773834,80.27905202399201,97.1104021449387,94.5338662651742,85.37533561604442,85.33384435026997,90.01104846248914,91.63722149395925,95.2503444964966,96.08895818605583,81.65781517635179,91.93465826064644,88.46761946090398,98.5207507186579,99.25687575947299,98.69585498631184,87.72631284189427,98.64060649545208,89.2204074423298,95.01164477343498,95.44005154374588,89.04936274459344,87.1869517635474,99.72246508351223,83.33799496361839,90.2450186476562,88.59649687838261,96.97083846952444,94.08873101079597,98.19822773301094,95.30946423582891,81.89486919642309,87.88279401615023,94.2009159920444,90.69343670005375,95.32808743808704,88.09423156567543,83.28134344069294,83.5803118258434,83.04475476660699,87.85631505703941,92.426240586468,90.31416240496293,88.05257520367633,84.82307572694081,98.86455584477818,83.94375939945674,80.2984177603922,80.43822248975171,99.50809119832991,85.91454834680852,98.87952050121785,80.27304054006858,82.63949053621768,87.48077223037008,92.6011569757252,86.30170501410622,88.20889175953153,84.83885040326102,90.97767799850692,87.30550639676437,82.45733298797359,83.05243527958369,80.5651033016135,93.53834772350135,97.80972320723083,86.85365003451396,99.27952575385761,95.34726388012457,96.74888563145284,91.72934274655412,96.39292205604187,89.51894215585986,81.67674224665895,99.51610079242039,88.7435663444366,92.74511402522853,92.82245164266405,96.60680993912457,95.78701148322514,80.13472562389875,88.65719471034565,85.66318822109008,95.98540628792853,91.71110668088379,82.73094631919004,94.36543449881133,86.39475924127275,86.32453040925087,83.20325969831774,88.40982866884839,81.04614423424009,81.00891165602542,92.22678479773751,93.72158385837984,89.55670841437995,91.83611257581518,99.2243463878155,99.4258206997753,88.62694007100905,83.93619261888124,89.4108736540542,94.04133627264241,80.2494775354143,92.84954645326108,98.18025790829033,99.48894676871068,91.33898187332838,98.74374921569505,99.31145759282333,91.57493113563746,94.86459021782156,98.33397360551668,91.24933658652299,88.66607156267132,97.85080074985723,81.57077313285382,86.5078712523109,92.65005358333624,83.33525711739586,89.11150822000832,95.86747797650213,88.25480828073289,95.33786372433705,90.50688528595536,93.07548580460833,92.5966957813082,96.7890938757154,82.89479968018783,85.88034421569097,84.56666569279051,89.13332307406112,87.55698428417529,92.21258080317244,80.08087807209576,99.85347730077288,89.56050451310324,83.19629272592607,89.10965733501561,96.3500970900018,84.50631903456718,87.00727780534727,90.81011246378296,84.80001731190322,88.35260492594908,93.78200676356121,93.42763514671839,81.97994900170764,89.48843971255398,91.354114005625,98.21635181987808,98.31196173341124,93.81963091561911,85.67747018797006,80.38079627329824,95.75219233879241,98.321501864174,90.77258488387606,83.00963133753622,99.33373173630176,96.82778443743001,95.96122158707557,83.71201192170717,84.05787564326086,95.50065310837537,94.75880624562899,93.59826030909352,95.75442177465438,95.135667531123,87.1345146904347,99.45497399032898,99.76605221895215,93.47828086029027,83.33857293333772,99.0922539255202,89.54808526333912,80.66347221517879,95.83088392003104,82.06543470737635,93.07958016394116,90.00444924063747,86.73379266420915,94.23763508660598,88.59511063189862,80.35170893929266,83.72098566588429,80.72748679478329,86.57743789824426,82.74181727966403,80.29027166447486,84.9649274597322,95.89817918804378,97.52513448536466,99.4626416425717,91.83856605620372,80.64924562156591,93.28401452203556,80.84327669622603,91.76699656792026,98.95632141426493,87.07864325563382,98.96158977755303,89.66742114148168,99.97522606177789,83.72115707753335,84.88009161119237,85.10335433369431,93.28966668612983,83.81532026865469,81.39825890815467,83.56713804092404,90.8101811633069,99.89090149379548,96.8545304549686,92.52420096809462,99.6348384970687,88.6248140261714,88.58401697991988,87.17443718338752,81.62612712427283,94.07211077634037,91.53593344961892,99.03621328283447,80.68088557331234,89.45750912596708,96.39928117917702,89.30153849561846,88.71052341627245,91.66735766320957,94.55368728575857,84.8056584383018,88.08274073493601,99.85096826417231,89.84458087470335,90.69490908991789,80.00991891252261,91.78310983066893,84.85874054643548,96.91385718641862,96.35925447257267,90.0265743872332,82.75233877198691,93.61918871036933,97.53184570746697,82.44053447685104,99.95174928186754,90.54884215425831,84.20297126209641,90.8779046657427,88.26231890030883,86.58039670922057,80.91756361672111,94.77628774602968,99.55308003966961,98.19314263429752,99.86262974637161,83.79195159815625,89.5289078243483,98.73057829500812,80.39356625069308,84.84138426416156,80.94872450351703,93.15155792387372,93.50901554809903,98.09091426193885,97.28510178067599,93.51405238112916,90.99102826098597,92.81208886119862,98.50894659640952,95.38561170096007,85.79331505381279,86.55698948117032,94.81311094756678,98.19204440638997,82.37876921050426,94.73086556565373,96.59008778713395,91.16174347112369,98.35027668626313,99.57064974233414,89.33078524308468,95.71801329567631,81.19546466653085,92.09591393050138,90.77719998647692,83.10284857715712,85.13716192051956,93.2296736891601,81.21847418321431,83.9894040997622,92.92212604485567,86.63066395576774,82.96538603279639,93.14588361585379,86.5701523572134,84.35668130274874,94.16827770246913,84.04038119854914,95.32287801121663,89.90662087326209,80.75688102191029,87.10757533822343,90.94582354672914,90.09372621336364,92.8208581659743,83.75464910714064,82.0613584813492,95.63257023033519,82.55882699729268,85.50755547195827,85.9445124359066,84.6106999361912,82.45438969613001,91.18448602002636,95.80211874190358,93.62342702316661,87.6438727768498,89.1522287595362,91.74619622166932,97.0130697307511,96.55206046880174,82.3615448334782,82.56790142255409,84.99527129797195,96.13706358389786,93.8809147830798,83.15306747608118,93.16790336746911,96.60183555433662,93.11892274472076,92.11260032120767,93.31082492087134,83.38492070528842,87.53176756286247,82.88038810034753,92.08592393840752,95.30501347569398,89.0476761431988,89.45662557914751,96.08666887893884,97.78984922496542,89.16762525948168,93.14055038042349,81.9205403986418,96.82942235617405,86.48667814810896,82.50909824407195,88.12868489398507,94.88126502736888,88.08104715632948,83.50145565829875,91.81897017273415,93.44768809582827,88.90684998200517,86.27002100215607,82.17246404857646,84.14653800119979,99.4803538668578,91.05399437636446,85.81535169797907],\"xaxis\":\"x\",\"y\":[90.01085570822246,90.18569700838155,89.98500635485917,90.65545254963425,88.94566697776055,89.57288240576102,90.48691135697817,89.04694454789943,90.04927191076847,90.52269654922266,90.92561490898726,89.06406350882756,90.36430058938167,89.32570643032497,90.1079115463707,91.14213203326474,90.25947532653493,90.70926371847087,89.5735661390928,90.79974647604277,91.6010537430315,91.02395950055333,91.07380150575588,89.50899516932915,89.58900607588227,90.61630308145303,89.55282862952626,90.3904194247918,90.71806137668251,90.1989673156885,89.14212342972012,90.54637140846071,89.62972876239868,89.37569783756445,90.50138445153473,89.89014797390783,88.75663371664989,91.76960936919969,89.28432294066434,90.74699328945664,91.58049117801336,90.05341246886515,90.07382135158055,90.16811037058842,89.88453598930782,90.35855705654885,90.89397757464225,89.38998594689875,89.9893833414129,90.31806343916625,90.20635855649225,90.49338664999554,89.0952820705779,89.71164950366591,89.39162246628176,90.24285847348062,89.0071199017555,91.46613456920315,89.91135856044336,90.88149852425849,88.41469604644172,90.38199708804402,90.11925725448687,89.62790683215269,89.50577287577467,89.77677443032418,90.27652683440718,90.0771926854604,88.72518276031569,89.69517679699376,89.91405394559813,88.95575664459103,90.66811648005546,89.85788079443167,89.40785642745932,89.81817345250866,89.12501227588254,89.6013714716109,89.61029613813832,89.47092422909255,92.2002584800522,90.45746864186529,90.51999903970207,89.28048810189722,89.64490465356694,89.76243915830658,89.17268741586692,89.58178393215226,89.50518190662585,88.93834634237705,90.06528306579435,89.91462232568033,89.3966944034631,90.25120841084944,90.76051573815855,88.73155986835434,91.68710980873757,89.12373213202491,89.71871988571296,91.07791352667178,89.91479823892935,90.78062346442255,90.02496052196268,90.3066070541798,89.80796521666588,89.50327481307231,88.54561435254912,91.32630014288769,91.00306488103702,89.94789398802112,90.11357919697738,89.56626388285744,90.28873951360694,90.3447341903814,89.27282708653749,89.26243120979557,89.81497362425253,91.7620301265116,89.14325730782156,90.57187717971685,89.2737863602021,89.44295018240146,89.61707494673627,89.10049138124907,90.73554027296169,90.17880877936739,91.0989335246085,89.80251887990747,90.01838307164671,90.70454929043332,89.06045688052662,90.21960534207658,90.92971288625861,91.09532072944113,91.75342173425571,91.0847702738065,89.59252211610311,90.06455916918009,91.3912445750345,90.87237365093742,90.24619327431591,90.0962126420479,89.50291436853523,89.71791161626666,90.11519387520896,90.2483869581691,90.52223993815689,90.19404942215577,89.51671626926267,89.35719349092003,90.82203366745102,90.05776632693929,90.11294509351731,90.56196048920584,90.56865880866658,90.13201821884239,90.12452700154888,89.80345484775543,89.38008805375301,90.06620317362275,90.02549252081607,90.28250482598935,90.36399620824334,90.19172715857539,90.58497384280494,90.43262105392674,89.78102248666924,88.8720668026856,89.94527522120225,89.44137024842095,90.18884057328539,90.5177343626447,90.33022517514216,89.85435459063955,90.10529696370716,89.44875607022318,90.84997636160003,91.22783969643183,90.72682722205514,90.53198301020436,89.20263973674815,92.70764032532112,89.97539441838717,90.69125298461967,88.90219010270685,89.94728551521446,89.72215692227961,92.21882245774057,91.02593442132073,89.97338920830707,91.22062450943613,89.29493706411007,90.22838247685421,90.81458766883279,90.68706491153412,90.35207724481629,89.57238935972738,89.66219217219722,90.22546511938319,89.67269487239453,89.67091242410682,91.20593690411431,89.3182022667869,90.00878757953845,90.9186457189285,89.86921409719345,89.14052264200153,89.42226338023988,89.0779232272005,90.90837195312784,89.6505527100759,89.35550867557242,89.52299249696652,90.22069391445866,90.11909878244046,89.01048522561382,89.99994485793454,90.83457141009244,89.6826673515707,89.289272926179,88.67305844705754,89.90925693421482,89.822741703654,90.13143023457853,89.87120582818756,90.03596357163906,89.7188334897655,91.07146285079868,90.3907689192625,88.83706420361095,90.8346776801784,89.01518192513427,89.64959295133558,89.62990246485496,89.07068062484657,90.50110349714356,90.0415102651832,90.53860032182452,90.32841719006476,90.32859736418446,90.17443980671722,88.91643461851541,89.9505551829537,89.33625638445666,88.9558710263423,90.42242331021656,90.84212972207386,89.73816151362223,91.32600258595835,88.8015227621422,89.2470429762566,90.07049728233756,89.62936113074304,90.05072448726821,90.12655064682568,90.0111602368139,89.92583720136136,90.24408005720406,90.47983333014545,89.96827081783275,88.8268607063493,91.48753589564726,89.92987151205192,91.1862539437657,91.32032145547547,90.0120878537133,90.29424504511391,90.83122950870045,90.61090575292265,90.08098006170306,89.08574448829157,90.37316610577312,88.97426988976171,90.44322182119585,89.61885409063066,89.30168680224264,90.43840732840343,89.75552539306894,90.17794818388549,89.98737140668752,89.27331927040099,90.16632897528739,90.27156216635858,91.26321276977338,89.62440899158986,90.21563490485558,89.53666887817158,90.51666444246015,90.04366399502213,88.70805376254455,89.14733358759176,90.33712600367046,91.2163030088742,89.12854285998237,90.5119080670069,90.01847939868931,89.84497317465056,90.51710754977861,89.60891841408879,89.53975342146524,90.00462175884766,91.24883786363907,90.80765955893324,89.22226244032919,90.22103817388978,89.4634386968468,89.28898887493831,89.85306161644675,89.94944493836958,91.13962728022344,89.11081446165912,88.79113399277865,89.79011564120849,90.02111277637451,90.94729813885192,91.04087912726403,88.8169877323813,90.07520713471538,90.54395124084795,89.12824763952435,89.46841305415892,89.39939754078546,89.83258130939677,90.02464968660342,89.64815101915705,89.77109598411973,89.17789474025392,91.09032989353166,90.3037020304549,90.25428563375874,88.36839974495237,90.27571307258168,91.11462186364739,88.70554878947817,89.52027042693975,88.94635575109041,90.3313996397223,90.96651987616761,91.1697753883393,89.48718123494127,91.14443908827258,89.00850266938153,89.73126242272268,89.72616219556514,88.83381761489207,92.22474539630868,88.43855978538946,89.48186881707151,88.59083227266444,89.4419004966157,89.83459410803405,90.75849478044955,90.45912684797796,90.87820072316495,89.5919157094963,90.68042098260119,89.51271425673599,90.82635599904883,90.08286948966743,90.33236250055835,89.67649253547387,90.67991184617507,90.15972733580736,89.85251430710039,89.77713597924216,90.8493292617101,90.4897802469537,89.67773687559077,89.25502385280006,89.51094808190173,92.08109277586502,92.46448117563848,90.32659724791327,89.02545060460088,89.98657704885176,88.144901994638,89.44476484411697,89.32011569573203,90.3725867801514,89.34402525269712,89.76189496690286,90.55934118296281,89.81889989236424,90.35505181099165,88.7522202706079,89.59453970569682,88.93108604147395,89.62164056891886,91.08905235659662,90.38803704453423,89.21762747226009,91.3491448216949,89.55173594132063,91.15703878709711,89.6376809216853,89.73128282033016,88.60737721100153,89.63238736150169,90.42083360850344,90.29235353627925,88.37744871269152,91.06904271890001,89.24753220416999,90.70232721831401,89.65081850570773,90.41339258252398,91.07459093828712,90.6336628586169,89.94189847655794,91.67427358795106,89.5249848325337,90.17658836996958,89.25465844833802,89.56391007390502,89.71570114447393,89.39513821936373,91.34786357670028,89.17175893717224,89.49431996072033,89.53923900259477,88.78317330178801,89.7830210513011,90.05373983171751,89.58341904901434,90.29192105325659,89.01873419061778,90.46185471707125,90.99275159724212,89.69887919806027,90.36482038498525,89.21354832743965,91.67050881541068,89.50804221890871,90.70730368039831,91.2997245646055,89.61691018299021,89.45239013837639,89.39532965193021,90.05656786190704,90.39886802037284,90.02156215427492,89.50214725203348,90.58317346677639,90.1946854750243,89.43433902247094,89.46544017703631,90.19562150094131,89.19164082155108,91.06608676630636,91.14052171891385,89.47788023885883,89.79421120161435,90.25995907133776,90.15178805538147,89.58464121730317,88.6080815841647,88.98212620261805,90.43861761566237,90.73235251701314,89.39508110645309,88.77833795425468,88.49456129434525,89.38251970006692,90.68237178236461,90.10567326980215,91.06800832420178,90.52140931722258,90.30132409769001,91.58578011944738,90.46819500767137,89.51865048081228,89.61100562697959,90.51142384417479,89.30869657100979,88.99528427829772,88.64529532356678,89.15581568814677,90.01467819780967,89.79957491650461,90.39208120846351,89.11208568692555,90.86243049762437,90.04741563193862,89.2702705756681,89.48134397915167,90.45122765185417,89.93526319405719,90.4410588297784,89.15664295733842,88.77863733581351,90.42893612745688,88.65843131630226,89.92957724331583,89.3017213234157,89.07133953913328,91.38099501817649,89.40205471929224,88.87859071031855,89.71727316113748,90.68346898343934,88.52761451669662,89.58600908712049,89.79913976698631,89.61944910670044,89.56640052561386,88.73570186233162,91.1527212881048,89.3980392207056,90.08977384680134,90.61372519192948,91.50553476187335,90.72203484682925,88.86230284471034,89.61307211918316,90.23160200926506,90.00059175197809,89.6041710947029,89.65630397193874,89.79386565973012,90.32449593350785,90.61824434341091,90.69061824052474,90.07882784944758,90.22494044320084,89.31804099368482,91.07417345868973,88.60454118010705,90.20713104809349,89.31707842392458,90.65912926347991,89.58705920340013,89.91628634157448,90.87591112520792,90.37845082902717,89.67401799407608,90.27016030249526,89.4145224831246,91.34307334882116,90.4250675935838,89.50075124828868,89.74650614430647,90.67850525820639,89.24997015489438,89.11050100575737,90.74047525628744,88.7340451430786,90.60435980889997,89.66924061250188,90.23445505052705,90.01804586206991,91.27911137980243,89.96799812082729,90.11532881014912,89.37011354263626,89.42804245253656,90.49798076278248,90.90953164175201,89.5705921881972,88.98577521546929,89.61749571002296,89.7950258772826,90.65628743278653,90.28536535929,90.25738043393866,90.06395293567785,88.85993682334762,89.66595741500512,88.71146587730756,89.67245259239921,89.01392052689073,91.53190201426824,90.58704420924735,89.58573330102459,89.61976647197312,89.58884749394568,88.64694765151621,91.06494253418505,90.08410274018777,90.27441090447805,90.98165904016084,89.69996992827275,90.44887358086046,90.08111815034765,91.45173613414111,90.8897862343517,88.88961502794635,89.1331512802924,91.09201657591613,89.69879943362301,89.05228230214381,89.82745977250713,88.7000548969982,89.5912549839749,90.0727014150561,89.22903004536279,89.06286744606453,90.82423869391363,90.69800218703669,91.59802280787487,90.23842810168647,91.50219652381301,89.91129008501316,91.02740017267581,90.55844202972742,90.48427368220912,90.84468072311557,89.87395104237942,89.65208533916065,90.64354737041522,89.45271472157484,90.98735452972517,89.39796288593352,91.08376294377403,89.27938148262777,90.12067420162282,90.63727773567089,89.94003769974518,90.81375211336265,90.2074769540383,89.26032118059909,88.93528664917588,90.18167876517433,90.12958382816258,89.2490103819874,89.30593893603712,90.20652458593149,91.27599616292449,90.82331155934098,91.44819084575195,90.00693128499094,89.30287874786404,89.39678471776833,90.4907946223047,88.71029809433534,89.07043222132462,89.86551162123394,90.67858773963658,89.35963179793131,90.53787243157404,90.8123826634496,90.20811420815865,90.0386512498318,90.53906738607597,90.71382297517003,89.54868848939483,90.37754578134727,89.96588005028133,90.77981863712998,89.11591971384489,91.10708689831736,89.40932372102503,89.7189137150628,89.78103542001318,92.01569600834637,90.71410155445747,89.91719822962264,90.51142797091549,89.03053757631889,89.5889306883494,90.55491989008272,89.72005100591049,90.52225051238915,90.15057122639516,91.27535657592941,89.69710269003131,89.49990960844241,90.56998703259913,89.70024590453451,90.62889973862194,89.16347123606178,90.85948474194568,92.03563151356556,90.78448300998976,90.1925552183334,90.23936586825553,90.47461825472327,90.71063482153423,89.00590544971446,89.9466198060417,88.74357368100455,89.930580713994,90.41165617036276,90.38923319686066,90.39327230102808,88.54764901775026,91.04920720849768,89.69208434192126,89.54043274815865,89.85013196314506,90.3379240231952,91.01956527421248,88.11051584922693,89.75391034423617,90.50841244521136,90.39178683008569,89.4741911406979,89.5203544466311,89.02883552929009,90.89254208916097,89.8359445646239,88.82930276982083,90.47699966218872,90.26342933972735,90.43877642144716,90.53845720585777,90.44673314301129,90.26312757328444,89.18150671466972,88.85125230768955,89.91874913085663,89.49533006656608,90.76404006505447,89.43699013690305,89.60280858269928,89.99770200229611,90.7088142405144,90.53522337657039,90.63204346182565,89.95845558131285,89.45848390089347,91.1021366370966,88.83554106139908,90.39730606242206,89.56708837328505,90.66791687300302,91.17496915409347,90.72522727747435,88.67917455673182,90.18290852919893,89.3673748323781,90.09794099217659,90.75703837120076,88.91926608319902,91.31279635065783,90.15063847273875,89.86535828104977,90.55956063046219,90.01263881392259,89.18038415690596,89.38756884708448,89.16166765264875,91.78702130466036,88.66865302044008,90.53753184961923,89.89484915097943,89.67457245917777,90.44180459414181,91.37930040773766,89.42251853680918,90.04904373616765,90.22720119240839,89.81283808660615,89.26612632936603,89.74807900249189,89.02101228529033,89.24493644973754,90.65089583667277,90.01808654078977,91.06816629298126,90.01712714869066,88.59435306182613,89.57950802843092,89.75077271118488,90.60964185460604,89.20563750919511,90.73158456791342,90.69787730791613,89.94933170596235,92.54802899247271,89.43436748107518,89.00364521185695,91.25937209610503,89.32391481020136,88.80905721442494,90.38054520276488,89.43258701381956,89.27178989641482,91.08871745756987,89.80934143702875,89.70662715787097,89.77636768459936,89.61715965276471,91.06181697909544,89.4066517419731,89.13622117024,91.02839171206631,89.60896071947914,91.73245319201781,89.41641176267906,90.17684054566713,90.03808691909762,90.48342133946039,90.93580969986552,90.2023839507887,89.71892458882375,90.1486405872384,89.41762673775608,89.80721699648414,90.75656535443886,88.92945529441235,89.37238789864,89.4532174678374,90.94576143263441,90.32904865978045,88.56108009521733,90.05422647596204,89.35352109831595,89.92879618006025,89.40055454699528,90.06876625404435,88.9571104897,90.10359462709366,89.61229597508883,89.80464613173169,91.10087953519364,91.13353955947224,89.76165837156805,90.2856608218704,89.56169937747735,88.95614078805409,91.7674520856433,89.52581475063594,89.83645608513314,91.14129458313252,89.28109657634653,89.65006454541584,91.39833391785271,89.55111310149924,89.85620712732289,90.76408724339856,90.22949427632815,89.42138945979195,90.19677762899298,89.70161967860051,90.45021718689004,89.39969554448831,88.33095019698318,90.98296731069759,90.31332434193139,89.39276000908565,90.6457362637993,90.74918021727184,90.48978018289945,89.207873632108,90.39579133783775,90.78648907299466,90.3798062148956,90.14829135299392,90.29612835599801,90.56181788124039,91.34934454114448,89.65392190044766,90.775678651773,89.79470009534158,90.94107432409298,89.11464512029929,90.20356525354855,90.56374412784139,90.46712824850036,89.02240467202083,88.62780287000147,90.47101846678375,89.26760271202568,90.77899424965926,88.95580295451842,91.02005288202226,89.37659904727245,91.02534845196803,90.3736520821821,91.31643612609233,90.53742600620471,89.32330442899043,91.44689583527534,88.61139216566806,90.62445924072597,89.80260256339206,90.87337849522531,89.78068451347161,89.50422549897323,89.92094633760607,90.91022519891814,90.3124334463466,88.76702840099469,89.72000307878841,90.07259563378493,90.11330525021967,89.53803804528896,90.11717605202222,91.87070908033171,88.95807326327609,90.8877582836647,89.58292580338767,89.4705172254683,90.6759975784655,88.28534386348886,90.66328054258213,90.0679737248817,90.57518027265485,89.15024559022982,90.62334422218686,90.48152271387985,87.8135995752622,90.88265189639841,89.67763592237554,89.66168221068988,89.4661246458319,91.8563288950642,90.17973401767601,90.74207657567811,90.32403064926413,89.20157864276435,90.57221968376379,89.76400990901575,89.90806430792246,89.78913649850656,90.49998562854739,90.19578449278659,90.26888713569831,91.51050896914852,90.75657946769914,90.03975941828068,90.20871667059195,89.44057027703109,89.98283324022063,90.35455121867884,89.21139564730049,90.79358283095435,89.45769410819041,88.62040592395269,90.04798294434374,89.66502638074651,91.08384186397174,90.65688171722294,89.25356754138738,90.40024488637366,90.24502563196185,89.72555916113427,89.57385150979687,90.80157559784736,88.50686037666978,89.95521832958525,91.3182844741567,89.69454212038782,90.91917106111678,90.15968518683178,89.41584060625333,90.06177974735265,90.74617997581747,90.50715467794078,88.97645182655455,89.67246290276859,89.79525854224802,89.54494934032269,89.44846941049691,90.05194834287245,90.85333593575967,90.31282637733712,90.46274975201342,89.44554863322281,89.69767414830207,89.3391901858663,90.25176937433685,90.18998051409994,89.55062450188318,89.64960586967808,90.32674309501995,89.70053492416943,90.33396643928401,89.7840656101502,90.90997532545443,91.002999639892,90.89850159956316,89.98268193716038,89.83011625393593,88.81277430101977,89.39688410877172,89.60905607554753,89.14749694857699,88.70521367786458,91.54375037436964,90.33628626737543,89.06200464998044,89.61383470616929,91.22924302429163,90.54491585102596,90.63068943234143,88.47776025937011,89.81281466832218,89.263296081925,89.38715679533763,89.07183174021195,90.65557227870042,89.45884285721654,90.0944826677279,90.09147190492797,88.88256014554773,90.72640785619652,89.1167528323913,89.74337358680728,89.22622520852937],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Perfect Prediction\",\"x\":[80.00195801390282,99.97522606177789],\"y\":[80.00195801390282,99.97522606177789],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Actual Efficiency\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Predicted Efficiency\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Efficiency Prediction Performance\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('170a7e99-54a3-4eb5-a871-b635ff04949e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Efficiency Prediction - MSE: 34.4476, R2 Score: -0.0062\n",
            "\n",
            "Recommendations:\n",
            "- Optimize Leakage_Rate control. Implement adaptive process control.\n",
            "- Optimize Compression_Force control. Implement adaptive process control.\n",
            "- Optimize Cell_Voltage control. Implement adaptive process control.\n",
            "- Optimize Welding_Time control. Implement adaptive process control.\n",
            "- Optimize Assembly_Time control. Implement adaptive process control.\n",
            "- Optimize Welding_Current control. Implement adaptive process control.\n",
            "- Optimize Production_Rate control. Implement adaptive process control.\n",
            "- Optimize Cell_Capacity control. Implement adaptive process control.\n",
            "- Optimize Torque control. Implement adaptive process control.\n",
            "- Optimize Room_Temperature control. Implement adaptive process control.\n",
            "\n",
            "Detecting errors and suggesting improvements...\n",
            "Cell_Voltage out of range in 56 samples.\n",
            "  - Check cell_voltage control systems and process parameters.\n",
            "Cell_Impedance out of range in 244 samples.\n",
            "  - Check cell_impedance control systems and process parameters.\n",
            "Cell_Capacity out of range in 10 samples.\n",
            "  - Check cell_capacity control systems and process parameters.\n",
            "Compression_Force out of range in 29 samples.\n",
            "  - Check compression_force control systems and process parameters.\n",
            "Welding_Current out of range in 3 samples.\n",
            "  - Check welding_current control systems and process parameters.\n",
            "Assembly_Time out of range in 5 samples.\n",
            "  - Check assembly_time control systems and process parameters.\n",
            "Leakage_Rate out of range in 714 samples.\n",
            "  - Check leakage_rate control systems and process parameters.\n",
            "Inconsistent welding energy detected.\n",
            "  - Review welding process parameters and equipment calibration.\n",
            "Unusual energy density detected in some cells.\n",
            "  - Verify cell specifications and quality control processes.\n",
            "\n",
            "Running a what-if scenario...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:38,323] A new study created in memory with name: no-name-e2021dd0-aa1d-4567-8ca9-1475ae3ab0fe\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What-if scenario result: {'defect_probability': {'No_Defect': 3, 'Minor_Defect': 0, 'Major_Defect': 0, 'Critical_Defect': 0}, 'efficiency_score': 89.04355971029936}\n",
            "\n",
            "Optimizing welding process...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:10:38,487] Trial 0 finished with value: 289.01626967881487 and parameters: {'Welding_Current': 282.20925407109445}. Best is trial 0 with value: 289.01626967881487.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:38,672] Trial 1 finished with value: 289.2703938630766 and parameters: {'Welding_Current': 157.01831606995458}. Best is trial 1 with value: 289.2703938630766.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:38,825] Trial 2 finished with value: 288.87518139379927 and parameters: {'Welding_Current': 190.43733596648033}. Best is trial 1 with value: 289.2703938630766.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:38,962] Trial 3 finished with value: 288.6688245021916 and parameters: {'Welding_Current': 231.08830728372968}. Best is trial 1 with value: 289.2703938630766.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:39,103] Trial 4 finished with value: 288.8130105723653 and parameters: {'Welding_Current': 264.98847156489467}. Best is trial 1 with value: 289.2703938630766.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:39,245] Trial 5 finished with value: 289.8500725738322 and parameters: {'Welding_Current': 129.2947881980384}. Best is trial 5 with value: 289.8500725738322.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:39,381] Trial 6 finished with value: 288.9773460337953 and parameters: {'Welding_Current': 278.86902604386734}. Best is trial 5 with value: 289.8500725738322.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:39,535] Trial 7 finished with value: 288.9224882420454 and parameters: {'Welding_Current': 179.33467798950764}. Best is trial 5 with value: 289.8500725738322.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:39,726] Trial 8 finished with value: 290.27232102188003 and parameters: {'Welding_Current': 112.16254864290084}. Best is trial 8 with value: 290.27232102188003.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:39,884] Trial 9 finished with value: 288.8886375377016 and parameters: {'Welding_Current': 269.71875840958614}. Best is trial 8 with value: 290.27232102188003.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,028] Trial 10 finished with value: 290.46910599502456 and parameters: {'Welding_Current': 104.03499646620251}. Best is trial 10 with value: 290.46910599502456.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,168] Trial 11 finished with value: 290.26919144424335 and parameters: {'Welding_Current': 112.29184549686613}. Best is trial 10 with value: 290.46910599502456.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,312] Trial 12 finished with value: 290.52121933730973 and parameters: {'Welding_Current': 101.88260091463856}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,447] Trial 13 finished with value: 289.40925610163265 and parameters: {'Welding_Current': 148.9789828328594}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,609] Trial 14 finished with value: 290.45054215225116 and parameters: {'Welding_Current': 104.80163728947014}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,797] Trial 15 finished with value: 288.69251907327646 and parameters: {'Welding_Current': 217.091200398127}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:40,938] Trial 16 finished with value: 289.50780296123827 and parameters: {'Welding_Current': 143.06967585886446}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:41,086] Trial 17 finished with value: 289.07767469851296 and parameters: {'Welding_Current': 168.35065078946468}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:41,234] Trial 18 finished with value: 289.71084569205107 and parameters: {'Welding_Current': 132.49642943266355}. Best is trial 12 with value: 290.52121933730973.\n",
            "<ipython-input-5-603f2f176d0a>:302: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:10:41,380] Trial 19 finished with value: 290.5101338270558 and parameters: {'Welding_Current': 102.34054030874594}. Best is trial 12 with value: 290.52121933730973.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Optimization result: {'optimal_value': 101.88260091463856, 'predicted_outcome': {'defect_probability': {'No_Defect': 3, 'Minor_Defect': 0, 'Major_Defect': 0, 'Critical_Defect': 0}, 'efficiency_score': 90.52121933730973}}\n"
          ]
        }
      ],
      "source": [
        "class EVModuleAssemblyAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.models = {}\n",
        "        self.scaler = None\n",
        "        self.feature_names = None\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def generate_data(self, num_samples=5000):  # Reduced from 50000 to 5000\n",
        "        np.random.seed(42)\n",
        "        self.data = pd.DataFrame({\n",
        "            'Assembly_Stage': np.random.choice(ASSEMBLY_STAGES, num_samples),\n",
        "            'Cell_Type': np.random.choice(CELL_TYPES, num_samples),\n",
        "            'Welding_Method': np.random.choice(WELDING_METHODS, num_samples),\n",
        "            'Cell_Voltage': np.random.normal(3.7, 0.2, num_samples),\n",
        "            'Cell_Impedance': np.random.normal(1.0, 0.3, num_samples),\n",
        "            'Cell_Capacity': np.random.normal(3500, 500, num_samples),\n",
        "            'Compression_Force': np.random.normal(1000, 200, num_samples),\n",
        "            'Welding_Current': np.random.normal(200, 30, num_samples),\n",
        "            'Welding_Time': np.random.normal(0.5, 0.1, num_samples),\n",
        "            'Torque': np.random.normal(10, 1, num_samples),\n",
        "            'Assembly_Time': np.random.normal(20, 3, num_samples),\n",
        "            'Leakage_Rate': np.random.exponential(0.05, num_samples),\n",
        "            'Operator_ID': np.random.randint(1, 10, num_samples),  # Reduced from 50 to 10\n",
        "            'Equipment_ID': np.random.randint(1, 5, num_samples),  # Reduced from 20 to 5\n",
        "            'Raw_Material_Batch': np.random.randint(1, 20, num_samples),  # Reduced from 100 to 20\n",
        "            'Room_Temperature': np.random.normal(22, 2, num_samples),\n",
        "            'Room_Humidity': np.random.normal(50, 5, num_samples),\n",
        "            'Shift': np.random.choice(['Morning', 'Afternoon', 'Night'], num_samples),\n",
        "            'Production_Rate': np.random.uniform(50, 100, num_samples),\n",
        "            'Defect_Type': np.random.choice(DEFECT_TYPES, num_samples, p=[0.7, 0.15, 0.1, 0.05]),\n",
        "            'Efficiency_Score': np.random.uniform(80, 100, num_samples)\n",
        "        })\n",
        "\n",
        "    def optimize_hyperparameters(self, model_type, X, y_defect, y_efficiency):\n",
        "        def objective(trial):\n",
        "            if model_type in ['rf', 'xgb', 'lgbm', 'catboost']:\n",
        "                if model_type == 'rf':\n",
        "                    params = {\n",
        "                        'n_estimators': trial.suggest_int('n_estimators', 50, 200),  # Reduced from 100-1000 to 50-200\n",
        "                        'max_depth': trial.suggest_int('max_depth', 3, 10),  # Reduced from 5-30 to 3-10\n",
        "                        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),  # Reduced from 2-20 to 2-10\n",
        "                        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5)  # Reduced from 1-10 to 1-5\n",
        "                    }\n",
        "                    model_defect = RandomForestClassifier(**params, random_state=42)\n",
        "                    model_efficiency = RandomForestRegressor(**params, random_state=42)\n",
        "                elif model_type == 'xgb':\n",
        "                    params = {\n",
        "                        'max_depth': trial.suggest_int('max_depth', 3, 8),  # Reduced from 3-10 to 3-8\n",
        "                        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
        "                        'n_estimators': trial.suggest_int('n_estimators', 50, 200),  # Reduced from 100-1000 to 50-200\n",
        "                        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),  # Reduced from 1-10 to 1-5\n",
        "                        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
        "                    }\n",
        "                    model_defect = XGBClassifier(**params, random_state=42)\n",
        "                    model_efficiency = XGBRegressor(**params, random_state=42)\n",
        "                elif model_type == 'lgbm':\n",
        "                    params = {\n",
        "                        'num_leaves': trial.suggest_int('num_leaves', 20, 50),  # Reduced from 20-100 to 20-50\n",
        "                        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
        "                        'n_estimators': trial.suggest_int('n_estimators', 50, 200),  # Reduced from 100-1000 to 50-200\n",
        "                        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),  # Reduced from 5-100 to 5-50\n",
        "                        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0)\n",
        "                    }\n",
        "                    model_defect = LGBMClassifier(**params, random_state=42)\n",
        "                    model_efficiency = LGBMRegressor(**params, random_state=42)\n",
        "                else:  # catboost\n",
        "                    params = {\n",
        "                        'depth': trial.suggest_int('depth', 4, 8),  # Reduced from 4-10 to 4-8\n",
        "                        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.5),  # Reduced upper bound from 1.0 to 0.5\n",
        "                        'iterations': trial.suggest_int('iterations', 50, 200)  # Reduced from 100-1000 to 50-200\n",
        "                    }\n",
        "                    model_defect = CatBoostClassifier(**params, random_state=42, verbose=0)\n",
        "                    model_efficiency = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
        "\n",
        "                defect_score = cross_val_score(model_defect, X, y_defect, cv=3, scoring='accuracy').mean()  # Reduced from cv=5 to cv=3\n",
        "                efficiency_score = -cross_val_score(model_efficiency, X, y_efficiency, cv=3, scoring='neg_mean_squared_error').mean()  # Reduced from cv=5 to cv=3\n",
        "\n",
        "                return defect_score - efficiency_score  # Optimize both objectives\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model type\")\n",
        "\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(objective, n_trials=20)  # Reduced from 100 to 20\n",
        "\n",
        "        return study.best_params\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        df_encoded = pd.get_dummies(self.data, columns=['Assembly_Stage', 'Cell_Type', 'Welding_Method', 'Shift'])\n",
        "        X = df_encoded.drop(['Defect_Type', 'Efficiency_Score'], axis=1)\n",
        "        y_defect = df_encoded['Defect_Type']\n",
        "        y_efficiency = df_encoded['Efficiency_Score']\n",
        "\n",
        "        # Encode the defect labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        y_defect_encoded = self.label_encoder.fit_transform(y_defect)\n",
        "\n",
        "        X_train, X_test, y_defect_train, y_defect_test, y_efficiency_train, y_efficiency_test = train_test_split(\n",
        "            X, y_defect_encoded, y_efficiency, test_size=0.2, random_state=42)\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        self.feature_names = X.columns\n",
        "        return X_train_scaled, X_test_scaled, y_defect_train, y_defect_test, y_efficiency_train, y_efficiency_test\n",
        "\n",
        "\n",
        "    def build_multi_input_nn(self, input_dim):\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        hidden1 = Dense(64, activation='relu')(input_layer)\n",
        "        hidden2 = Dense(32, activation='relu')(hidden1)\n",
        "\n",
        "        defect_output = Dense(len(DEFECT_TYPES), activation='softmax', name='defect_output')(hidden2)\n",
        "        efficiency_output = Dense(1, name='efficiency_output')(hidden2)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=[defect_output, efficiency_output])\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss={'defect_output': 'categorical_crossentropy', 'efficiency_output': 'mse'},\n",
        "                      metrics={'defect_output': 'accuracy', 'efficiency_output': 'mse'})\n",
        "        return model\n",
        "\n",
        "\n",
        "    def train_models(self, X_train, y_defect_train, y_efficiency_train):\n",
        "        for model_type in ['rf', 'xgb', 'lgbm', 'catboost']:\n",
        "            params = self.optimize_hyperparameters(model_type, X_train, y_defect_train, y_efficiency_train)\n",
        "\n",
        "            if model_type == 'rf':\n",
        "                self.models[f'{model_type}_defect'] = RandomForestClassifier(**params, random_state=42)\n",
        "                self.models[f'{model_type}_efficiency'] = RandomForestRegressor(**params, random_state=42)\n",
        "            elif model_type == 'xgb':\n",
        "                self.models[f'{model_type}_defect'] = XGBClassifier(**params, random_state=42)\n",
        "                self.models[f'{model_type}_efficiency'] = XGBRegressor(**params, random_state=42)\n",
        "            elif model_type == 'lgbm':\n",
        "                self.models[f'{model_type}_defect'] = LGBMClassifier(**params, random_state=42)\n",
        "                self.models[f'{model_type}_efficiency'] = LGBMRegressor(**params, random_state=42)\n",
        "            elif model_type == 'catboost':\n",
        "                self.models[f'{model_type}_defect'] = CatBoostClassifier(**params, random_state=42, verbose=0)\n",
        "                self.models[f'{model_type}_efficiency'] = CatBoostRegressor(**params, random_state=42, verbose=0)\n",
        "\n",
        "            self.models[f'{model_type}_defect'].fit(X_train, y_defect_train)\n",
        "            self.models[f'{model_type}_efficiency'].fit(X_train, y_efficiency_train)\n",
        "\n",
        "        self.models['nn'] = self.build_multi_input_nn(X_train.shape[1])\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        self.models['nn'].fit(X_train,\n",
        "                              {'defect_output': pd.get_dummies(y_defect_train), 'efficiency_output': y_efficiency_train},\n",
        "                              epochs=30, batch_size=32, validation_split=0.2,\n",
        "                              callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "\n",
        "    def ensemble_predict(self, X):\n",
        "        defect_predictions = []\n",
        "        efficiency_predictions = []\n",
        "\n",
        "        for model_type in ['rf', 'xgb', 'lgbm', 'catboost']:\n",
        "            defect_pred = self.models[f'{model_type}_defect'].predict(X)\n",
        "            efficiency_pred = self.models[f'{model_type}_efficiency'].predict(X)\n",
        "            defect_predictions.append(defect_pred)\n",
        "            efficiency_predictions.append(efficiency_pred)\n",
        "\n",
        "        nn_defect_pred, nn_efficiency_pred = self.models['nn'].predict(X)\n",
        "        nn_defect_pred = np.argmax(nn_defect_pred, axis=1)\n",
        "        defect_predictions.append(nn_defect_pred)\n",
        "        efficiency_predictions.append(nn_efficiency_pred.flatten())\n",
        "\n",
        "        ensemble_defect_pred = stats.mode(defect_predictions, axis=0)[0].flatten()\n",
        "        ensemble_efficiency_pred = np.mean(efficiency_predictions, axis=0)\n",
        "\n",
        "        return ensemble_defect_pred, ensemble_efficiency_pred\n",
        "\n",
        "    def detect_anomalies(self, data, contamination=0.01):\n",
        "        clf = IsolationForest(contamination=contamination, random_state=42)\n",
        "        data_numeric = data.select_dtypes(include=np.number)\n",
        "        anomalies = clf.fit_predict(data_numeric) == -1\n",
        "        return data[anomalies]\n",
        "\n",
        "\n",
        "\n",
        "    def ensemble_predict(self, X):\n",
        "        defect_predictions = []\n",
        "        efficiency_predictions = []\n",
        "\n",
        "        for model_type in ['rf', 'xgb', 'lgbm', 'catboost']:\n",
        "            defect_pred = self.models[f'{model_type}_defect'].predict(X)\n",
        "            efficiency_pred = self.models[f'{model_type}_efficiency'].predict(X)\n",
        "            defect_predictions.append(defect_pred)\n",
        "            efficiency_predictions.append(efficiency_pred)\n",
        "\n",
        "        nn_defect_pred, nn_efficiency_pred = self.models['nn'].predict(X)\n",
        "        nn_defect_pred = np.argmax(nn_defect_pred, axis=1)\n",
        "        defect_predictions.append(nn_defect_pred)\n",
        "        efficiency_predictions.append(nn_efficiency_pred.flatten())\n",
        "\n",
        "        # Ensure all predictions have the same shape\n",
        "        defect_predictions = [pred.reshape(-1, 1) if pred.ndim == 1 else pred for pred in defect_predictions]\n",
        "        efficiency_predictions = [pred.reshape(-1, 1) if pred.ndim == 1 else pred for pred in efficiency_predictions]\n",
        "\n",
        "        ensemble_defect_pred = stats.mode(defect_predictions, axis=0)[0].flatten()\n",
        "        ensemble_efficiency_pred = np.mean(efficiency_predictions, axis=0).flatten()\n",
        "\n",
        "        return ensemble_defect_pred, ensemble_efficiency_pred\n",
        "\n",
        "    def analyze_and_recommend(self, X):\n",
        "        defect_predictions, efficiency_predictions = self.ensemble_predict(X)\n",
        "        feature_importance = self.models['rf_defect'].feature_importances_\n",
        "\n",
        "        sorted_idx = np.argsort(feature_importance)\n",
        "        top_features = self.feature_names[sorted_idx[-10:]]\n",
        "\n",
        "        recommendations = []\n",
        "        for feature in top_features:\n",
        "            if 'Assembly_Stage' in feature:\n",
        "                stage = feature.split('_')[-1]\n",
        "                recommendations.append(f\"High defect risk detected in {stage} stage. Review process controls.\")\n",
        "            elif 'Cell_Type' in feature:\n",
        "                cell_type = feature.split('_')[-1]\n",
        "                recommendations.append(f\"{cell_type} cells significantly impact defects. Enhance quality control.\")\n",
        "            elif 'Welding_Method' in feature:\n",
        "                method = feature.split('_')[-1]\n",
        "                recommendations.append(f\"Optimize {method} welding parameters. Implement real-time monitoring.\")\n",
        "            else:\n",
        "                recommendations.append(f\"Optimize {feature} control. Implement adaptive process control.\")\n",
        "\n",
        "        return recommendations, feature_importance, defect_predictions, efficiency_predictions\n",
        "\n",
        "    def visualize_results(self, feature_importance, y_true_defect, y_pred_defect, y_true_efficiency, y_pred_efficiency):\n",
        "        # Feature importance plot\n",
        "        sorted_idx = np.argsort(feature_importance)\n",
        "        top_features = self.feature_names[sorted_idx[-10:]]\n",
        "        top_importance = feature_importance[sorted_idx[-10:]]\n",
        "        fig = px.bar(x=top_importance, y=top_features, orientation='h', title='Top 10 Feature Importance')\n",
        "        fig.show()\n",
        "\n",
        "        # Confusion matrix for defect prediction\n",
        "        cm = confusion_matrix(y_true_defect, y_pred_defect)\n",
        "        defect_labels = self.label_encoder.classes_\n",
        "        fig = px.imshow(cm, labels=dict(x=\"Predicted\", y=\"Actual\"), x=defect_labels, y=defect_labels,\n",
        "                        title='Confusion Matrix - Defect Prediction', color_continuous_scale='Blues')\n",
        "        fig.update_xaxes(side=\"top\")\n",
        "        fig.show()\n",
        "\n",
        "        # Efficiency prediction scatter plot\n",
        "        fig = px.scatter(x=y_true_efficiency, y=y_pred_efficiency,\n",
        "                         labels={'x': 'Actual Efficiency', 'y': 'Predicted Efficiency'},\n",
        "                         title='Efficiency Prediction Performance')\n",
        "        fig.add_trace(go.Scatter(x=[min(y_true_efficiency), max(y_true_efficiency)],\n",
        "                                 y=[min(y_true_efficiency), max(y_true_efficiency)],\n",
        "                                 mode='lines', name='Perfect Prediction'))\n",
        "        fig.show()\n",
        "\n",
        "        # Print efficiency prediction metrics\n",
        "        mse = mean_squared_error(y_true_efficiency, y_pred_efficiency)\n",
        "        r2 = r2_score(y_true_efficiency, y_pred_efficiency)\n",
        "        print(f\"Efficiency Prediction - MSE: {mse:.4f}, R2 Score: {r2:.4f}\")\n",
        "\n",
        "    def what_if_scenario(self, scenario_changes):\n",
        "        \"\"\"\n",
        "        Run a what-if scenario by modifying specific features and predicting the outcome.\n",
        "\n",
        "        :param scenario_changes: dict, keys are feature names and values are the new values\n",
        "        :return: dict with predicted defect probability and efficiency score\n",
        "        \"\"\"\n",
        "        # Create a copy of the last row in the dataset\n",
        "        base_scenario = self.data.iloc[-1:].copy()\n",
        "\n",
        "        # Apply the changes\n",
        "        for feature, value in scenario_changes.items():\n",
        "            if feature in base_scenario.columns:\n",
        "                base_scenario[feature] = value\n",
        "            else:\n",
        "                print(f\"Warning: Feature '{feature}' not found in the dataset.\")\n",
        "\n",
        "        # Preprocess the scenario data\n",
        "        scenario_encoded = pd.get_dummies(base_scenario, columns=['Assembly_Stage', 'Cell_Type', 'Welding_Method', 'Shift'])\n",
        "        scenario_encoded = scenario_encoded.reindex(columns=self.feature_names, fill_value=0)\n",
        "        scenario_scaled = self.scaler.transform(scenario_encoded)\n",
        "\n",
        "        # Make predictions\n",
        "        defect_prob, efficiency_score = self.ensemble_predict(scenario_scaled)\n",
        "\n",
        "        # Handle both single-value and array predictions\n",
        "        if np.isscalar(defect_prob):\n",
        "            defect_prob = [defect_prob]\n",
        "        if np.isscalar(efficiency_score):\n",
        "            efficiency_score = [efficiency_score]\n",
        "\n",
        "        # Ensure defect_prob has the same length as DEFECT_TYPES\n",
        "        if len(defect_prob) < len(DEFECT_TYPES):\n",
        "            defect_prob = np.pad(defect_prob, (0, len(DEFECT_TYPES) - len(defect_prob)), 'constant')\n",
        "        elif len(defect_prob) > len(DEFECT_TYPES):\n",
        "            defect_prob = defect_prob[:len(DEFECT_TYPES)]\n",
        "\n",
        "        return {\n",
        "            'defect_probability': {DEFECT_TYPES[i]: defect_prob[i] for i in range(len(DEFECT_TYPES))},\n",
        "            'efficiency_score': efficiency_score[0]\n",
        "        }\n",
        "\n",
        "    def optimize_process(self, target_feature, constraints=None):\n",
        "        def objective(trial):\n",
        "            scenario = {target_feature: trial.suggest_uniform(target_feature,\n",
        "                                                              THRESHOLDS[target_feature][0],\n",
        "                                                              THRESHOLDS[target_feature][1])}\n",
        "            if constraints:\n",
        "                scenario.update(constraints)\n",
        "\n",
        "            result = self.what_if_scenario(scenario)\n",
        "            defect_score = 1 - result['defect_probability']['No_Defect']\n",
        "            efficiency_score = result['efficiency_score']\n",
        "\n",
        "            return efficiency_score - 100 * defect_score  # Penalize defects heavily\n",
        "\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(objective, n_trials=20)  # Reduced from 100 to 20\n",
        "\n",
        "        optimal_scenario = {target_feature: study.best_params[target_feature]}\n",
        "        if constraints:\n",
        "            optimal_scenario.update(constraints)\n",
        "\n",
        "        optimal_result = self.what_if_scenario(optimal_scenario)\n",
        "\n",
        "        return {\n",
        "            'optimal_value': study.best_params[target_feature],\n",
        "            'predicted_outcome': optimal_result\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    def detect_errors_and_suggest(self):\n",
        "        errors = []\n",
        "        for feature, (low, high) in THRESHOLDS.items():\n",
        "            if feature in self.data.columns:\n",
        "                out_of_range = self.data[(self.data[feature] < low) | (self.data[feature] > high)]\n",
        "                if not out_of_range.empty:\n",
        "                    errors.append(f\"{feature} out of range in {len(out_of_range)} samples.\")\n",
        "                    errors.append(f\"  - Check {feature.lower()} control systems and process parameters.\")\n",
        "\n",
        "        # Check for correlations specific to EV module assembly\n",
        "        if 'Welding_Current' in self.data.columns and 'Welding_Time' in self.data.columns:\n",
        "            welding_energy = self.data['Welding_Current'] * self.data['Welding_Time']\n",
        "            if (welding_energy < 20).any() or (welding_energy > 300).any():\n",
        "                errors.append(\"Inconsistent welding energy detected.\")\n",
        "                errors.append(\"  - Review welding process parameters and equipment calibration.\")\n",
        "\n",
        "        if 'Cell_Voltage' in self.data.columns and 'Cell_Capacity' in self.data.columns:\n",
        "            energy_density = self.data['Cell_Voltage'] * self.data['Cell_Capacity']\n",
        "            if (energy_density < 7000).any() or (energy_density > 25000).any():\n",
        "                errors.append(\"Unusual energy density detected in some cells.\")\n",
        "                errors.append(\"  - Verify cell specifications and quality control processes.\")\n",
        "\n",
        "        return errors\n",
        "\n",
        "    def run_analysis(self):\n",
        "        print(\"Generating and preprocessing data...\")\n",
        "        self.generate_data()\n",
        "        X_train, X_test, y_defect_train, y_defect_test, y_efficiency_train, y_efficiency_test = self.preprocess_data()\n",
        "\n",
        "        print(\"Training models...\")\n",
        "        self.train_models(X_train, y_defect_train, y_efficiency_train)\n",
        "\n",
        "        print(\"Detecting anomalies...\")\n",
        "        anomalies = self.detect_anomalies(self.data)\n",
        "        print(f\"Detected {len(anomalies)} anomalies.\")\n",
        "\n",
        "        print(\"Analyzing and generating recommendations...\")\n",
        "        recommendations, feature_importance, defect_predictions, efficiency_predictions = self.analyze_and_recommend(X_test)\n",
        "\n",
        "        print(\"Visualizing results...\")\n",
        "        self.visualize_results(feature_importance, y_defect_test, defect_predictions, y_efficiency_test, efficiency_predictions)\n",
        "\n",
        "        print(\"\\nRecommendations:\")\n",
        "        for rec in recommendations:\n",
        "            print(f\"- {rec}\")\n",
        "\n",
        "        print(\"\\nDetecting errors and suggesting improvements...\")\n",
        "        errors = self.detect_errors_and_suggest()\n",
        "        for error in errors:\n",
        "            print(error)\n",
        "\n",
        "        print(\"\\nRunning a what-if scenario...\")\n",
        "        scenario_result = self.what_if_scenario({'Welding_Current': 250, 'Welding_Time': 0.7})\n",
        "        print(\"What-if scenario result:\", scenario_result)\n",
        "\n",
        "        print(\"\\nOptimizing welding process...\")\n",
        "        optimization_result = self.optimize_process('Welding_Current', constraints={'Welding_Time': 0.5})\n",
        "        print(\"Optimization result:\", optimization_result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = EVModuleAssemblyAnalyzer()\n",
        "    analyzer.run_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intializing EVModuleAssemblyAnalyzer Class\n",
        "\n",
        "This code initializes an EVModuleAssemblyAnalyzer, generates and preprocesses data, trains models, and then sets up a Flask web application with various routes for model performance analysis, feature importance visualization, anomaly detection, what-if scenarios, process optimization, and error detection. The app is configured to run on a random port and provide a public URL for access."
      ],
      "metadata": {
        "id": "GDUSC6FGMmxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fbReqsPXU_Ww",
        "outputId": "be0063d9-4c6d-48fe-8fe4-a107bd09c29b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:24:49,576] A new study created in memory with name: no-name-b33829b1-7b82-4f55-9340-5c06c7fe4160\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating and preprocessing data...\n",
            "Training models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:24:55,720] Trial 0 finished with value: -32.311645510899325 and parameters: {'n_estimators': 63, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: -32.311645510899325.\n",
            "[I 2024-09-19 17:25:12,035] Trial 1 finished with value: -32.42511313415713 and parameters: {'n_estimators': 125, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: -32.311645510899325.\n",
            "[I 2024-09-19 17:25:16,859] Trial 2 finished with value: -32.02350218556482 and parameters: {'n_estimators': 80, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 2 with value: -32.02350218556482.\n",
            "[I 2024-09-19 17:25:28,568] Trial 3 finished with value: -32.122696671680224 and parameters: {'n_estimators': 145, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 2 with value: -32.02350218556482.\n",
            "[I 2024-09-19 17:25:47,232] Trial 4 finished with value: -32.49219354108488 and parameters: {'n_estimators': 142, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 2 with value: -32.02350218556482.\n",
            "[I 2024-09-19 17:25:56,170] Trial 5 finished with value: -32.01242034420776 and parameters: {'n_estimators': 161, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:26:06,364] Trial 6 finished with value: -32.524466104878556 and parameters: {'n_estimators': 79, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:26:29,062] Trial 7 finished with value: -32.39317017804203 and parameters: {'n_estimators': 174, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:26:33,510] Trial 8 finished with value: -32.02090101859129 and parameters: {'n_estimators': 89, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:26:39,782] Trial 9 finished with value: -32.09505238347431 and parameters: {'n_estimators': 84, 'max_depth': 4, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:26:57,872] Trial 10 finished with value: -32.19724215052595 and parameters: {'n_estimators': 199, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:27:03,679] Trial 11 finished with value: -32.01311381595915 and parameters: {'n_estimators': 110, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:27:12,715] Trial 12 finished with value: -32.076566988515204 and parameters: {'n_estimators': 129, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:27:25,759] Trial 13 finished with value: -32.115937314575326 and parameters: {'n_estimators': 162, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:27:31,722] Trial 14 finished with value: -32.01921193628193 and parameters: {'n_estimators': 112, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:27:42,032] Trial 15 finished with value: -32.24979109274024 and parameters: {'n_estimators': 110, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:27:53,565] Trial 16 finished with value: -32.0691071886487 and parameters: {'n_estimators': 169, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:28:14,053] Trial 17 finished with value: -32.25947831437613 and parameters: {'n_estimators': 196, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:28:22,464] Trial 18 finished with value: -32.14194728822538 and parameters: {'n_estimators': 102, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:28:25,163] Trial 19 finished with value: -32.061901673773804 and parameters: {'n_estimators': 53, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 5 with value: -32.01242034420776.\n",
            "[I 2024-09-19 17:28:29,804] A new study created in memory with name: no-name-c0fac81f-c795-4db4-b05e-6d17ab272af6\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:28:44,007] Trial 0 finished with value: -31.885532725654254 and parameters: {'max_depth': 8, 'learning_rate': 0.0012964928206373196, 'n_estimators': 133, 'min_child_weight': 2, 'subsample': 0.8952210993906051}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:28:46,493] Trial 1 finished with value: -31.91387782658318 and parameters: {'max_depth': 4, 'learning_rate': 0.003140683453413227, 'n_estimators': 105, 'min_child_weight': 2, 'subsample': 0.9816221073858964}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:28:51,636] Trial 2 finished with value: -34.31071912608805 and parameters: {'max_depth': 5, 'learning_rate': 0.06118127736875883, 'n_estimators': 154, 'min_child_weight': 3, 'subsample': 0.6214597267607305}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:28:57,422] Trial 3 finished with value: -32.57547803729373 and parameters: {'max_depth': 3, 'learning_rate': 0.03479727215229167, 'n_estimators': 192, 'min_child_weight': 4, 'subsample': 0.8418954263395872}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:01,655] Trial 4 finished with value: -32.606415366925695 and parameters: {'max_depth': 5, 'learning_rate': 0.027952467535971998, 'n_estimators': 116, 'min_child_weight': 2, 'subsample': 0.9017490033798257}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:04,329] Trial 5 finished with value: -31.89575892889682 and parameters: {'max_depth': 4, 'learning_rate': 0.002303117897307364, 'n_estimators': 121, 'min_child_weight': 4, 'subsample': 0.9306880504474896}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:05,709] Trial 6 finished with value: -32.135808353843586 and parameters: {'max_depth': 4, 'learning_rate': 0.02705824172814839, 'n_estimators': 58, 'min_child_weight': 5, 'subsample': 0.776127271274171}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:15,440] Trial 7 finished with value: -35.920648036480166 and parameters: {'max_depth': 5, 'learning_rate': 0.09575175640191438, 'n_estimators': 197, 'min_child_weight': 2, 'subsample': 0.760234386287403}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:18,495] Trial 8 finished with value: -32.017997026347516 and parameters: {'max_depth': 4, 'learning_rate': 0.007057146172722364, 'n_estimators': 136, 'min_child_weight': 4, 'subsample': 0.8790261183744381}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:37,703] Trial 9 finished with value: -31.95449638863356 and parameters: {'max_depth': 8, 'learning_rate': 0.0036508130540304955, 'n_estimators': 128, 'min_child_weight': 2, 'subsample': 0.6947022160935576}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:46,697] Trial 10 finished with value: -41.098834523116736 and parameters: {'max_depth': 8, 'learning_rate': 0.40045374190080313, 'n_estimators': 85, 'min_child_weight': 1, 'subsample': 0.9890969111866059}. Best is trial 0 with value: -31.885532725654254.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:29:59,648] Trial 11 finished with value: -31.879414962283757 and parameters: {'max_depth': 7, 'learning_rate': 0.0010209793077535817, 'n_estimators': 160, 'min_child_weight': 4, 'subsample': 0.9242119882463049}. Best is trial 11 with value: -31.879414962283757.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:30:12,529] Trial 12 finished with value: -31.870390179950327 and parameters: {'max_depth': 7, 'learning_rate': 0.001055756413845362, 'n_estimators': 164, 'min_child_weight': 3, 'subsample': 0.8165283596573061}. Best is trial 12 with value: -31.870390179950327.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:30:24,466] Trial 13 finished with value: -31.86922997389445 and parameters: {'max_depth': 7, 'learning_rate': 0.001074676415415277, 'n_estimators': 166, 'min_child_weight': 5, 'subsample': 0.820696453782284}. Best is trial 13 with value: -31.86922997389445.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:30:36,784] Trial 14 finished with value: -32.23944700281264 and parameters: {'max_depth': 7, 'learning_rate': 0.009351325283472092, 'n_estimators': 170, 'min_child_weight': 5, 'subsample': 0.8205019150273539}. Best is trial 13 with value: -31.86922997389445.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:30:49,996] Trial 15 finished with value: -32.32069313559189 and parameters: {'max_depth': 7, 'learning_rate': 0.009916970198303338, 'n_estimators': 178, 'min_child_weight': 3, 'subsample': 0.7377563430608333}. Best is trial 13 with value: -31.86922997389445.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:30:57,504] Trial 16 finished with value: -31.873443083645448 and parameters: {'max_depth': 6, 'learning_rate': 0.001931807609411038, 'n_estimators': 144, 'min_child_weight': 5, 'subsample': 0.7122157886257036}. Best is trial 13 with value: -31.86922997389445.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:31:07,264] Trial 17 finished with value: -32.02936562269271 and parameters: {'max_depth': 6, 'learning_rate': 0.005095557162244222, 'n_estimators': 180, 'min_child_weight': 3, 'subsample': 0.8323803370906561}. Best is trial 13 with value: -31.86922997389445.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:31:19,230] Trial 18 finished with value: -32.55909557815872 and parameters: {'max_depth': 7, 'learning_rate': 0.013373302979384435, 'n_estimators': 158, 'min_child_weight': 3, 'subsample': 0.663548043850812}. Best is trial 13 with value: -31.86922997389445.\n",
            "<ipython-input-5-603f2f176d0a>:50: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:53: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n",
            "[I 2024-09-19 17:31:26,688] Trial 19 finished with value: -38.686349603469814 and parameters: {'max_depth': 6, 'learning_rate': 0.23589625378793694, 'n_estimators': 89, 'min_child_weight': 1, 'subsample': 0.7895644342338536}. Best is trial 13 with value: -31.86922997389445.\n",
            "[I 2024-09-19 17:31:30,002] A new study created in memory with name: no-name-dcddac69-dae3-4437-9319-6fc62cb80ef8\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000473 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:31:34,898] Trial 0 finished with value: -37.98534844919467 and parameters: {'num_leaves': 21, 'learning_rate': 0.2414331870872899, 'n_estimators': 191, 'min_child_samples': 5, 'subsample': 0.7291668519503334}. Best is trial 0 with value: -37.98534844919467.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:31:39,809] Trial 1 finished with value: -31.88129715379774 and parameters: {'num_leaves': 39, 'learning_rate': 0.0018035934300640187, 'n_estimators': 86, 'min_child_samples': 48, 'subsample': 0.6030583825746492}. Best is trial 1 with value: -31.88129715379774.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:31:44,381] Trial 2 finished with value: -32.828348494591694 and parameters: {'num_leaves': 37, 'learning_rate': 0.016963325208325324, 'n_estimators': 126, 'min_child_samples': 47, 'subsample': 0.6865649654478954}. Best is trial 1 with value: -31.88129715379774.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000540 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:31:52,924] Trial 3 finished with value: -38.79407037205189 and parameters: {'num_leaves': 37, 'learning_rate': 0.20343899917009906, 'n_estimators': 194, 'min_child_samples': 27, 'subsample': 0.9179936229034221}. Best is trial 1 with value: -31.88129715379774.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:31:59,285] Trial 4 finished with value: -33.97630100213526 and parameters: {'num_leaves': 28, 'learning_rate': 0.03811987001665938, 'n_estimators': 196, 'min_child_samples': 26, 'subsample': 0.9358981858467614}. Best is trial 1 with value: -31.88129715379774.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000482 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:01,512] Trial 5 finished with value: -39.493813293825944 and parameters: {'num_leaves': 39, 'learning_rate': 0.3168563267560007, 'n_estimators': 55, 'min_child_samples': 34, 'subsample': 0.9153110190537779}. Best is trial 1 with value: -31.88129715379774.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000458 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:08,190] Trial 6 finished with value: -35.6840077694352 and parameters: {'num_leaves': 43, 'learning_rate': 0.08546485547290529, 'n_estimators': 114, 'min_child_samples': 21, 'subsample': 0.9642066546350035}. Best is trial 1 with value: -31.88129715379774.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:11,864] Trial 7 finished with value: -35.03583484110124 and parameters: {'num_leaves': 48, 'learning_rate': 0.09642711425340596, 'n_estimators': 74, 'min_child_samples': 45, 'subsample': 0.9643187988198108}. Best is trial 1 with value: -31.88129715379774.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000527 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000463 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:15,293] Trial 8 finished with value: -36.88886621260242 and parameters: {'num_leaves': 39, 'learning_rate': 0.18205761569561, 'n_estimators': 82, 'min_child_samples': 19, 'subsample': 0.8009302172515078}. Best is trial 1 with value: -31.88129715379774.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:18,061] Trial 9 finished with value: -31.970451891260375 and parameters: {'num_leaves': 21, 'learning_rate': 0.003893068319630746, 'n_estimators': 113, 'min_child_samples': 37, 'subsample': 0.7158387543371707}. Best is trial 1 with value: -31.88129715379774.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000505 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000455 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:24,394] Trial 10 finished with value: -31.87587848282721 and parameters: {'num_leaves': 30, 'learning_rate': 0.001069096770282393, 'n_estimators': 152, 'min_child_samples': 50, 'subsample': 0.6503827569350523}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000501 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:29,245] Trial 11 finished with value: -31.887257898075646 and parameters: {'num_leaves': 29, 'learning_rate': 0.0010995559442422587, 'n_estimators': 154, 'min_child_samples': 49, 'subsample': 0.6198740530214357}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:35,100] Trial 12 finished with value: -31.882242100905472 and parameters: {'num_leaves': 30, 'learning_rate': 0.001074566091912036, 'n_estimators': 150, 'min_child_samples': 39, 'subsample': 0.6489212385270272}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:41,011] Trial 13 finished with value: -32.12945869640042 and parameters: {'num_leaves': 32, 'learning_rate': 0.004328510675031302, 'n_estimators': 157, 'min_child_samples': 50, 'subsample': 0.6018054675166695}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000495 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000683 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:44,926] Trial 14 finished with value: -31.9518225939566 and parameters: {'num_leaves': 45, 'learning_rate': 0.0032727753451550546, 'n_estimators': 94, 'min_child_samples': 42, 'subsample': 0.7661787569211786}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000453 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000472 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:46,471] Trial 15 finished with value: -31.940762583704746 and parameters: {'num_leaves': 25, 'learning_rate': 0.008205983926855204, 'n_estimators': 50, 'min_child_samples': 33, 'subsample': 0.838914422075251}. Best is trial 10 with value: -31.87587848282721.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000490 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000518 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:32:53,065] Trial 16 finished with value: -32.0306117475414 and parameters: {'num_leaves': 34, 'learning_rate': 0.0022245595164456194, 'n_estimators': 134, 'min_child_samples': 9, 'subsample': 0.6536230171942639}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000488 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:33:00,273] Trial 17 finished with value: -33.03235878172498 and parameters: {'num_leaves': 43, 'learning_rate': 0.012766484000027998, 'n_estimators': 174, 'min_child_samples': 43, 'subsample': 0.6709589856634703}. Best is trial 10 with value: -31.87587848282721.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:33:05,496] Trial 18 finished with value: -31.868924619677074 and parameters: {'num_leaves': 34, 'learning_rate': 0.0018293611600021106, 'n_estimators': 100, 'min_child_samples': 32, 'subsample': 0.6052820428389922}. Best is trial 18 with value: -31.868924619677074.\n",
            "<ipython-input-5-603f2f176d0a>:60: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "<ipython-input-5-603f2f176d0a>:63: FutureWarning:\n",
            "\n",
            "suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013137\n",
            "[LightGBM] [Info] Start training from score -2.275206\n",
            "[LightGBM] [Info] Start training from score -1.964079\n",
            "[LightGBM] [Info] Start training from score -0.345591\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.013512\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345966\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.021175\n",
            "[LightGBM] [Info] Start training from score -2.275581\n",
            "[LightGBM] [Info] Start training from score -1.961783\n",
            "[LightGBM] [Info] Start training from score -0.345436\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2666, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 89.907243\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000464 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.144141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:33:08,386] Trial 19 finished with value: -32.14365340727846 and parameters: {'num_leaves': 24, 'learning_rate': 0.007408019514295731, 'n_estimators': 102, 'min_child_samples': 13, 'subsample': 0.8625049834718034}. Best is trial 18 with value: -31.868924619677074.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000466 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 2667, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.161450\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score -3.015935\n",
            "[LightGBM] [Info] Start training from score -2.275456\n",
            "[LightGBM] [Info] Start training from score -1.962548\n",
            "[LightGBM] [Info] Start training from score -0.345664\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3137\n",
            "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 90.070965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 17:33:09,681] A new study created in memory with name: no-name-debc50c2-9fb9-4ae9-b73d-bbaea5626ea8\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:33:18,236] Trial 0 finished with value: -32.58888889316658 and parameters: {'depth': 7, 'learning_rate': 0.07972829148650595, 'iterations': 70}. Best is trial 0 with value: -32.58888889316658.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:33:24,886] Trial 1 finished with value: -40.58143934213201 and parameters: {'depth': 5, 'learning_rate': 0.46264534560142007, 'iterations': 191}. Best is trial 0 with value: -32.58888889316658.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:33:55,380] Trial 2 finished with value: -31.89021122029969 and parameters: {'depth': 8, 'learning_rate': 0.004386881098432745, 'iterations': 163}. Best is trial 2 with value: -31.89021122029969.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:34:05,530] Trial 3 finished with value: -31.880045204351227 and parameters: {'depth': 7, 'learning_rate': 0.005995918224280634, 'iterations': 87}. Best is trial 3 with value: -31.880045204351227.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:34:08,494] Trial 4 finished with value: -31.868812499443123 and parameters: {'depth': 4, 'learning_rate': 0.003431632305748187, 'iterations': 124}. Best is trial 4 with value: -31.868812499443123.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:34:15,756] Trial 5 finished with value: -32.08588318632012 and parameters: {'depth': 6, 'learning_rate': 0.03422530730618994, 'iterations': 93}. Best is trial 4 with value: -31.868812499443123.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:34:24,862] Trial 6 finished with value: -32.39269040711655 and parameters: {'depth': 7, 'learning_rate': 0.060505946923714714, 'iterations': 90}. Best is trial 4 with value: -31.868812499443123.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:34:46,576] Trial 7 finished with value: -34.63935390609954 and parameters: {'depth': 8, 'learning_rate': 0.16690133783532554, 'iterations': 108}. Best is trial 4 with value: -31.868812499443123.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:11,300] Trial 8 finished with value: -39.09936366841391 and parameters: {'depth': 7, 'learning_rate': 0.34589657017244146, 'iterations': 191}. Best is trial 4 with value: -31.868812499443123.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:19,620] Trial 9 finished with value: -32.31795571209142 and parameters: {'depth': 6, 'learning_rate': 0.03525272803642142, 'iterations': 151}. Best is trial 4 with value: -31.868812499443123.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:24,673] Trial 10 finished with value: -31.854480193291117 and parameters: {'depth': 4, 'learning_rate': 0.0010145615599701917, 'iterations': 134}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:27,971] Trial 11 finished with value: -31.854793588791395 and parameters: {'depth': 4, 'learning_rate': 0.0010582225912761532, 'iterations': 135}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:31,433] Trial 12 finished with value: -31.854699891829924 and parameters: {'depth': 4, 'learning_rate': 0.001015894107657355, 'iterations': 149}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:38,720] Trial 13 finished with value: -31.860031837049526 and parameters: {'depth': 5, 'learning_rate': 0.0012106450956830377, 'iterations': 157}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:41,786] Trial 14 finished with value: -31.912622211817457 and parameters: {'depth': 4, 'learning_rate': 0.009311286293691863, 'iterations': 129}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:43,783] Trial 15 finished with value: -31.855909285021262 and parameters: {'depth': 5, 'learning_rate': 0.0023068254588812947, 'iterations': 50}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:47,649] Trial 16 finished with value: -31.962110855198866 and parameters: {'depth': 4, 'learning_rate': 0.013254566330575122, 'iterations': 170}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:54,354] Trial 17 finished with value: -31.862367270112358 and parameters: {'depth': 5, 'learning_rate': 0.0018896787965246424, 'iterations': 143}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:35:57,127] Trial 18 finished with value: -31.86025722265233 and parameters: {'depth': 4, 'learning_rate': 0.001971988118081467, 'iterations': 115}. Best is trial 10 with value: -31.854480193291117.\n",
            "<ipython-input-5-603f2f176d0a>:70: FutureWarning:\n",
            "\n",
            "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "\n",
            "[I 2024-09-19 17:36:03,776] Trial 19 finished with value: -32.00626467180409 and parameters: {'depth': 5, 'learning_rate': 0.015331982907731098, 'iterations': 167}. Best is trial 10 with value: -31.854480193291117.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model training complete!\n",
            "Public URL: https://ccz88tuk3wv-496ff2e9c6d22116-6085-colab.googleusercontent.com/\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:6085\n",
            " * Running on http://172.28.0.12:6085\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Sep/2024 17:43:08] \"\u001b[33mGET /?authuser=0 HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Sep/2024 18:15:17] \"\u001b[33mGET /?authuser=0 HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Sep/2024 18:15:22] \"\u001b[33mGET /data?authuser=0 HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "analyzer = EVModuleAssemblyAnalyzer()\n",
        "\n",
        "print(\"Generating and preprocessing data...\")\n",
        "analyzer.generate_data()\n",
        "X_train, X_test, y_defect_train, y_defect_test, y_efficiency_train, y_efficiency_test = analyzer.preprocess_data()\n",
        "\n",
        "print(\"Training models...\")\n",
        "analyzer.train_models(X_train, y_defect_train, y_efficiency_train)\n",
        "\n",
        "print(\"Model training complete!\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/model_performance', methods=['GET'])\n",
        "def model_performance():\n",
        "    y_pred_defect, y_pred_efficiency = analyzer.ensemble_predict(X_test)\n",
        "\n",
        "    defect_accuracy = accuracy_score(y_defect_test, y_pred_defect)\n",
        "    efficiency_r2 = r2_score(y_efficiency_test, y_pred_efficiency)\n",
        "    efficiency_mse = mean_squared_error(y_efficiency_test, y_pred_efficiency)\n",
        "\n",
        "    return jsonify({\n",
        "        'defect_accuracy': defect_accuracy,\n",
        "        'efficiency_r2': efficiency_r2,\n",
        "        'efficiency_mse': efficiency_mse\n",
        "    })\n",
        "\n",
        "@app.route('/feature_importance', methods=['GET'])\n",
        "def feature_importance():\n",
        "    feature_importance = analyzer.models['rf_defect'].feature_importances_\n",
        "    sorted_idx = np.argsort(feature_importance)\n",
        "    top_features = analyzer.feature_names[sorted_idx[-10:]]\n",
        "    top_importance = feature_importance[sorted_idx[-10:]]\n",
        "\n",
        "    fig = px.bar(x=top_importance, y=top_features, orientation='h', title='Top 10 Feature Importance')\n",
        "    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n",
        "\n",
        "@app.route('/confusion_matrix', methods=['GET'])\n",
        "def confusion_matrix_data():\n",
        "    y_pred_defect, _ = analyzer.ensemble_predict(X_test)\n",
        "    cm = confusion_matrix(y_defect_test, y_pred_defect)\n",
        "    defect_labels = analyzer.label_encoder.classes_\n",
        "\n",
        "    fig = px.imshow(cm, labels=dict(x=\"Predicted\", y=\"Actual\"), x=defect_labels, y=defect_labels,\n",
        "                    title='Confusion Matrix - Defect Prediction', color_continuous_scale='Blues')\n",
        "    fig.update_xaxes(side=\"top\")\n",
        "    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n",
        "\n",
        "@app.route('/efficiency_scatter', methods=['GET'])\n",
        "def efficiency_scatter():\n",
        "    _, y_pred_efficiency = analyzer.ensemble_predict(X_test)\n",
        "\n",
        "    fig = px.scatter(x=y_efficiency_test, y=y_pred_efficiency,\n",
        "                     labels={'x': 'Actual Efficiency', 'y': 'Predicted Efficiency'},\n",
        "                     title='Efficiency Prediction Performance')\n",
        "    fig.add_trace(go.Scatter(x=[min(y_efficiency_test), max(y_efficiency_test)],\n",
        "                             y=[min(y_efficiency_test), max(y_efficiency_test)],\n",
        "                             mode='lines', name='Perfect Prediction'))\n",
        "    return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n",
        "\n",
        "@app.route('/anomalies', methods=['GET'])\n",
        "def anomaly_detection():\n",
        "    anomalies = analyzer.detect_anomalies(analyzer.data)\n",
        "    return jsonify({\n",
        "        'num_anomalies': len(anomalies),\n",
        "        'anomaly_percentage': (len(anomalies) / len(analyzer.data)) * 100\n",
        "    })\n",
        "\n",
        "@app.route('/what_if', methods=['POST'])\n",
        "def what_if_scenario():\n",
        "    data = request.json\n",
        "\n",
        "    # Map the input keys to the ones expected by the model\n",
        "    input_mapping = {\n",
        "        'cellVoltage': 'Cell_Voltage',\n",
        "        'cellImpedance': 'Cell_Impedance',\n",
        "        'cellCapacity': 'Cell_Capacity',\n",
        "        'compressionForce': 'Compression_Force',\n",
        "        'weldingCurrent': 'Welding_Current',\n",
        "        'weldingTime': 'Welding_Time',\n",
        "        'torque': 'Torque',\n",
        "        'assemblyTime': 'Assembly_Time',\n",
        "        'leakageRate': 'Leakage_Rate'\n",
        "    }\n",
        "\n",
        "    # Create the scenario dictionary with the correct keys\n",
        "    scenario = {input_mapping[key]: float(value) for key, value in data.items() if value}\n",
        "\n",
        "    # Run the what-if scenario\n",
        "    result = analyzer.what_if_scenario(scenario)\n",
        "\n",
        "    # Format the result for the front-end\n",
        "    formatted_result = {\n",
        "        'defectProbability': {k: float(v) for k, v in result['defect_probability'].items()},\n",
        "        'efficiencyScore': float(result['efficiency_score'])\n",
        "    }\n",
        "\n",
        "    return jsonify(formatted_result)\n",
        "\n",
        "@app.route('/optimize', methods=['POST'])\n",
        "def optimize_process():\n",
        "    data = request.json\n",
        "    target_feature = data.get('targetFeature')\n",
        "    constraints = data.get('constraints', {})\n",
        "\n",
        "    # Run the optimization\n",
        "    result = analyzer.optimize_process(target_feature, constraints)\n",
        "\n",
        "    # Format the result for the front-end\n",
        "    formatted_result = {\n",
        "        'optimalValue': float(result['optimal_value']),\n",
        "        'predictedOutcome': {\n",
        "            'defectProbability': {k: float(v) for k, v in result['predicted_outcome']['defect_probability'].items()},\n",
        "            'efficiencyScore': float(result['predicted_outcome']['efficiency_score'])\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return jsonify(formatted_result)\n",
        "\n",
        "@app.route('/error_detection', methods=['GET'])\n",
        "def error_detection():\n",
        "    errors = analyzer.detect_errors_and_suggest()\n",
        "    return jsonify({'errors': errors})\n",
        "\n",
        "def get_public_url(port):\n",
        "    return eval_js(f\"google.colab.kernel.proxyPort({port})\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    port = random.randint(5000, 9000)  # Random port number\n",
        "    public_url = get_public_url(port)\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    app.run(port=port, host='0.0.0.0')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}